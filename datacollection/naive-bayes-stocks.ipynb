{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b227128-c002-43d5-8ab0-7e47807b4c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73ff7bf2-30ed-404e-8b87-a5a70b943765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frame\n",
    "data = pd.read_csv('stock_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d71ce0be-1f99-4c55-8da0-35f00640d677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date   open   high     low  close    volume Name   7_day_ma  \\\n",
      "0 2013-02-08  45.07  45.35  45.000  45.08   1824755    A        NaN   \n",
      "1 2013-02-11  45.17  45.18  44.450  44.60   2915405    A        NaN   \n",
      "2 2013-02-12  44.81  44.95  44.500  44.62   2373731    A        NaN   \n",
      "3 2013-02-13  44.81  45.24  44.680  44.75   2052338    A        NaN   \n",
      "4 2013-02-14  44.72  44.78  44.360  44.58   3826245    A        NaN   \n",
      "5 2013-02-15  43.48  44.24  42.210  42.25  14657315    A        NaN   \n",
      "6 2013-02-19  42.21  43.12  42.210  43.01   4116141    A  44.127143   \n",
      "7 2013-02-20  42.84  42.85  42.225  42.24   3873183    A  43.721429   \n",
      "8 2013-02-21  42.14  42.14  41.470  41.63   3415149    A  43.297143   \n",
      "9 2013-02-22  41.83  42.07  41.580  41.80   3354862    A  42.894286   \n",
      "\n",
      "   15_day_ma  30_day_ma  daily_returns  daily_volatility  target  year  month  \\\n",
      "0        NaN        NaN            NaN               NaN       0  2013      2   \n",
      "1        NaN        NaN      -0.010648               NaN       0  2013      2   \n",
      "2        NaN        NaN       0.000448               NaN       1  2013      2   \n",
      "3        NaN        NaN       0.002913               NaN       1  2013      2   \n",
      "4        NaN        NaN      -0.003799               NaN       0  2013      2   \n",
      "5        NaN        NaN      -0.052266               NaN       0  2013      2   \n",
      "6        NaN        NaN       0.017988               NaN       1  2013      2   \n",
      "7        NaN        NaN      -0.017903               NaN       0  2013      2   \n",
      "8        NaN        NaN      -0.014441               NaN       0  2013      2   \n",
      "9        NaN        NaN       0.004084               NaN       1  2013      2   \n",
      "\n",
      "   day  \n",
      "0    8  \n",
      "1   11  \n",
      "2   12  \n",
      "3   13  \n",
      "4   14  \n",
      "5   15  \n",
      "6   19  \n",
      "7   20  \n",
      "8   21  \n",
      "9   22  \n"
     ]
    }
   ],
   "source": [
    "# transform date to datetime, get year,month,day from date column\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day\n",
    "\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfa5e4e6-41a0-4648-8956-73e2bd11ed9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date   open   high     low  close    volume   7_day_ma  15_day_ma  \\\n",
      "0 2013-02-08  45.07  45.35  45.000  45.08   1824755        NaN        NaN   \n",
      "1 2013-02-11  45.17  45.18  44.450  44.60   2915405        NaN        NaN   \n",
      "2 2013-02-12  44.81  44.95  44.500  44.62   2373731        NaN        NaN   \n",
      "3 2013-02-13  44.81  45.24  44.680  44.75   2052338        NaN        NaN   \n",
      "4 2013-02-14  44.72  44.78  44.360  44.58   3826245        NaN        NaN   \n",
      "5 2013-02-15  43.48  44.24  42.210  42.25  14657315        NaN        NaN   \n",
      "6 2013-02-19  42.21  43.12  42.210  43.01   4116141  44.127143        NaN   \n",
      "7 2013-02-20  42.84  42.85  42.225  42.24   3873183  43.721429        NaN   \n",
      "8 2013-02-21  42.14  42.14  41.470  41.63   3415149  43.297143        NaN   \n",
      "9 2013-02-22  41.83  42.07  41.580  41.80   3354862  42.894286        NaN   \n",
      "\n",
      "   30_day_ma  daily_returns  ...  Name_XL  Name_XLNX  Name_XOM  Name_XRAY  \\\n",
      "0        NaN            NaN  ...        0          0         0          0   \n",
      "1        NaN      -0.010648  ...        0          0         0          0   \n",
      "2        NaN       0.000448  ...        0          0         0          0   \n",
      "3        NaN       0.002913  ...        0          0         0          0   \n",
      "4        NaN      -0.003799  ...        0          0         0          0   \n",
      "5        NaN      -0.052266  ...        0          0         0          0   \n",
      "6        NaN       0.017988  ...        0          0         0          0   \n",
      "7        NaN      -0.017903  ...        0          0         0          0   \n",
      "8        NaN      -0.014441  ...        0          0         0          0   \n",
      "9        NaN       0.004084  ...        0          0         0          0   \n",
      "\n",
      "   Name_XRX  Name_XYL  Name_YUM  Name_ZBH  Name_ZION  Name_ZTS  \n",
      "0         0         0         0         0          0         0  \n",
      "1         0         0         0         0          0         0  \n",
      "2         0         0         0         0          0         0  \n",
      "3         0         0         0         0          0         0  \n",
      "4         0         0         0         0          0         0  \n",
      "5         0         0         0         0          0         0  \n",
      "6         0         0         0         0          0         0  \n",
      "7         0         0         0         0          0         0  \n",
      "8         0         0         0         0          0         0  \n",
      "9         0         0         0         0          0         0  \n",
      "\n",
      "[10 rows x 520 columns]\n"
     ]
    }
   ],
   "source": [
    "#save names of all companies prior to on-hotencoding\n",
    "unique_companies = data['Name'].unique()\n",
    "\n",
    "# One-hot encode company tickers to differentiate between companies in training\n",
    "data = pd.get_dummies(data, columns=['Name'])\n",
    "\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae12422c-df7d-4295-959a-30ce86398ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create columns with lag\n",
    "lag_columns = ['open', 'high', 'low', 'volume', '7_day_ma', '15_day_ma', '30_day_ma', 'daily_returns', 'daily_volatility']\n",
    "#for every company\n",
    "for ticker in unique_companies:\n",
    "    ticker_data = data.loc[data[f'Name_{ticker}'] == 1]  #get all data fora  specific company\n",
    "    for col in lag_columns: #for every column we're creating lag for\n",
    "        for n in [1, 3, 5,7,15,30]: #create lag for days 1,3,5,7,15,30\n",
    "            column_name = f'{col}_lag_{n}' #create new column with afforementioned lag\n",
    "            data.loc[data[f'Name_{ticker}'] == 1, column_name] = ticker_data[col].shift(n)#shift data by that many days\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec2cf4ed-6adf-4611-afce-164c6bbfaee8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date   open   high     low  close    volume   7_day_ma  15_day_ma  \\\n",
      "0 2013-02-08  45.07  45.35  45.000  45.08   1824755        NaN        NaN   \n",
      "1 2013-02-11  45.17  45.18  44.450  44.60   2915405        NaN        NaN   \n",
      "2 2013-02-12  44.81  44.95  44.500  44.62   2373731        NaN        NaN   \n",
      "3 2013-02-13  44.81  45.24  44.680  44.75   2052338        NaN        NaN   \n",
      "4 2013-02-14  44.72  44.78  44.360  44.58   3826245        NaN        NaN   \n",
      "5 2013-02-15  43.48  44.24  42.210  42.25  14657315        NaN        NaN   \n",
      "6 2013-02-19  42.21  43.12  42.210  43.01   4116141  44.127143        NaN   \n",
      "7 2013-02-20  42.84  42.85  42.225  42.24   3873183  43.721429        NaN   \n",
      "8 2013-02-21  42.14  42.14  41.470  41.63   3415149  43.297143        NaN   \n",
      "9 2013-02-22  41.83  42.07  41.580  41.80   3354862  42.894286        NaN   \n",
      "\n",
      "   30_day_ma  daily_returns  ...  daily_returns_lag_5  daily_returns_lag_7  \\\n",
      "0        NaN            NaN  ...                  NaN                  NaN   \n",
      "1        NaN      -0.010648  ...                  NaN                  NaN   \n",
      "2        NaN       0.000448  ...                  NaN                  NaN   \n",
      "3        NaN       0.002913  ...                  NaN                  NaN   \n",
      "4        NaN      -0.003799  ...                  NaN                  NaN   \n",
      "5        NaN      -0.052266  ...                  NaN                  NaN   \n",
      "6        NaN       0.017988  ...            -0.010648                  NaN   \n",
      "7        NaN      -0.017903  ...             0.000448                  NaN   \n",
      "8        NaN      -0.014441  ...             0.002913            -0.010648   \n",
      "9        NaN       0.004084  ...            -0.003799             0.000448   \n",
      "\n",
      "   daily_returns_lag_15  daily_returns_lag_30  daily_volatility_lag_1  \\\n",
      "0                   NaN                   NaN                     NaN   \n",
      "1                   NaN                   NaN                     NaN   \n",
      "2                   NaN                   NaN                     NaN   \n",
      "3                   NaN                   NaN                     NaN   \n",
      "4                   NaN                   NaN                     NaN   \n",
      "5                   NaN                   NaN                     NaN   \n",
      "6                   NaN                   NaN                     NaN   \n",
      "7                   NaN                   NaN                     NaN   \n",
      "8                   NaN                   NaN                     NaN   \n",
      "9                   NaN                   NaN                     NaN   \n",
      "\n",
      "   daily_volatility_lag_3  daily_volatility_lag_5  daily_volatility_lag_7  \\\n",
      "0                     NaN                     NaN                     NaN   \n",
      "1                     NaN                     NaN                     NaN   \n",
      "2                     NaN                     NaN                     NaN   \n",
      "3                     NaN                     NaN                     NaN   \n",
      "4                     NaN                     NaN                     NaN   \n",
      "5                     NaN                     NaN                     NaN   \n",
      "6                     NaN                     NaN                     NaN   \n",
      "7                     NaN                     NaN                     NaN   \n",
      "8                     NaN                     NaN                     NaN   \n",
      "9                     NaN                     NaN                     NaN   \n",
      "\n",
      "   daily_volatility_lag_15  daily_volatility_lag_30  \n",
      "0                      NaN                      NaN  \n",
      "1                      NaN                      NaN  \n",
      "2                      NaN                      NaN  \n",
      "3                      NaN                      NaN  \n",
      "4                      NaN                      NaN  \n",
      "5                      NaN                      NaN  \n",
      "6                      NaN                      NaN  \n",
      "7                      NaN                      NaN  \n",
      "8                      NaN                      NaN  \n",
      "9                      NaN                      NaN  \n",
      "\n",
      "[10 rows x 574 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3138a87-9f10-4bef-9a52-b9a88e1c1cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date   open    high     low  close    volume   7_day_ma  15_day_ma  \\\n",
      "60 2013-05-07  42.18  42.410  41.900  42.40   3524022  41.662857  41.933333   \n",
      "61 2013-05-08  42.40  42.950  42.300  42.94   2119765  41.874286  41.983333   \n",
      "62 2013-05-09  42.97  43.195  42.630  43.16   3159293  42.120000  42.072000   \n",
      "63 2013-05-10  43.12  43.850  43.040  43.63   4662252  42.451429  42.180667   \n",
      "64 2013-05-13  43.43  43.560  42.720  43.04   4260335  42.674286  42.260000   \n",
      "65 2013-05-14  42.98  44.060  42.882  43.97   6075845  43.020000  42.350667   \n",
      "66 2013-05-15  44.90  46.490  44.890  45.68  10289000  43.545714  42.539333   \n",
      "67 2013-05-16  45.43  45.840  44.970  44.99   4890962  43.915714  42.690000   \n",
      "68 2013-05-17  45.02  45.830  44.990  45.56   3247851  44.290000  42.974000   \n",
      "69 2013-05-20  45.48  47.450  45.390  46.34   5698804  44.744286  43.299333   \n",
      "\n",
      "    30_day_ma  daily_returns  ...  daily_returns_lag_5  daily_returns_lag_7  \\\n",
      "60  42.018333       0.009524  ...            -0.000482            -0.033466   \n",
      "61  42.055667       0.012736  ...            -0.003137             0.003874   \n",
      "62  42.091333       0.005123  ...             0.004115            -0.000482   \n",
      "63  42.146667       0.010890  ...             0.001688            -0.003137   \n",
      "64  42.217000      -0.013523  ...             0.010830             0.004115   \n",
      "65  42.327333       0.021608  ...             0.009524             0.001688   \n",
      "66  42.499333       0.038890  ...             0.012736             0.010830   \n",
      "67  42.618667      -0.015105  ...             0.005123             0.009524   \n",
      "68  42.752667       0.012669  ...             0.010890             0.012736   \n",
      "69  42.908667       0.017120  ...            -0.013523             0.005123   \n",
      "\n",
      "    daily_returns_lag_15  daily_returns_lag_30  daily_volatility_lag_1  \\\n",
      "60              0.008132             -0.002909                0.016187   \n",
      "61             -0.027656              0.016776                0.016249   \n",
      "62             -0.008533              0.006456                0.016132   \n",
      "63              0.004064             -0.002851                0.016118   \n",
      "64             -0.003571             -0.024780                0.016201   \n",
      "65              0.018160             -0.006597                0.015696   \n",
      "66              0.005632             -0.003443                0.016017   \n",
      "67             -0.002800              0.021964                0.017270   \n",
      "68             -0.033466              0.003139                0.017277   \n",
      "69              0.003874              0.002889                0.017369   \n",
      "\n",
      "    daily_volatility_lag_3  daily_volatility_lag_5  daily_volatility_lag_7  \\\n",
      "60                0.016785                0.017213                0.017317   \n",
      "61                0.016177                0.017067                0.017309   \n",
      "62                0.016187                0.016785                0.017213   \n",
      "63                0.016249                0.016177                0.017067   \n",
      "64                0.016132                0.016187                0.016785   \n",
      "65                0.016118                0.016249                0.016177   \n",
      "66                0.016201                0.016132                0.016187   \n",
      "67                0.015696                0.016118                0.016249   \n",
      "68                0.016017                0.016201                0.016132   \n",
      "69                0.017270                0.015696                0.016118   \n",
      "\n",
      "    daily_volatility_lag_15  daily_volatility_lag_30  \n",
      "60                 0.015512                 0.014528  \n",
      "61                 0.016143                 0.014885  \n",
      "62                 0.015994                 0.014960  \n",
      "63                 0.016020                 0.014934  \n",
      "64                 0.016008                 0.015500  \n",
      "65                 0.016352                 0.012400  \n",
      "66                 0.016362                 0.011862  \n",
      "67                 0.016289                 0.012236  \n",
      "68                 0.017317                 0.011968  \n",
      "69                 0.017309                 0.011956  \n",
      "\n",
      "[10 rows x 574 columns]\n"
     ]
    }
   ],
   "source": [
    "# drop NaN values\n",
    "data = data.dropna()\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c21c2dd6-d5bd-4d00-9619-1ac76a74647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop one-hot encoded columns\n",
    "columns_to_drop = [col for col in data.columns if col.startswith('Name_')]\n",
    "data = data.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16748b6b-d998-45e1-a4de-73a9c8e26d47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#define features, target vars\n",
    "\n",
    "#a list of columns to exclude. This should be all the columns with data from a day d for which we are trying \n",
    "#to make predictions for (because we will not have access to this data in practice)\n",
    "exclude = [col for col in lag_columns if any(f'{col}_lag_' in c for c in data.columns)]\n",
    "#a list of the features to include. This is all columns that are not the date, target, or included\n",
    "#in our list of columns to exclude\n",
    "features = [col for col in data.columns if col not in ['date', 'target'] + exclude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18926102-30aa-4974-9f27-e73ea577fb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into features and target\n",
    "X = data[features]\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bf28cc6-f881-48a7-b23a-924aab37c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TimeSeriesSplit\n",
    "# split the data into five folds in chronological order based on time.\n",
    "#first fold: (train)(test)\n",
    "#second fold: (train)(train)(test)\n",
    "#third fold: (train)(train)(train)(test)\n",
    "#etc...\n",
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "342dafde-dddc-4533-8680-c1f82093c231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.09      0.16     47136\n",
      "           1       0.52      0.92      0.66     50814\n",
      "\n",
      "    accuracy                           0.52     97950\n",
      "   macro avg       0.51      0.50      0.41     97950\n",
      "weighted avg       0.51      0.52      0.42     97950\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.13      0.21     47087\n",
      "           1       0.52      0.87      0.65     50863\n",
      "\n",
      "    accuracy                           0.52     97950\n",
      "   macro avg       0.50      0.50      0.43     97950\n",
      "weighted avg       0.50      0.52      0.44     97950\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.25      0.33     47112\n",
      "           1       0.52      0.76      0.62     50838\n",
      "\n",
      "    accuracy                           0.52     97950\n",
      "   macro avg       0.51      0.51      0.48     97950\n",
      "weighted avg       0.51      0.52      0.48     97950\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.74      0.59     47344\n",
      "           1       0.53      0.27      0.35     50606\n",
      "\n",
      "    accuracy                           0.50     97950\n",
      "   macro avg       0.51      0.51      0.47     97950\n",
      "weighted avg       0.51      0.50      0.47     97950\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.86      0.62     46853\n",
      "           1       0.53      0.15      0.23     51097\n",
      "\n",
      "    accuracy                           0.49     97950\n",
      "   macro avg       0.51      0.50      0.42     97950\n",
      "weighted avg       0.51      0.49      0.41     97950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for every fold get the training indice and testing indice\n",
    "#the training index will be an array used to get all data up to a point x and grows with each fold\n",
    "#the testing indices are all the data from a point x to a point y\n",
    "#the size of [x:y] is constant throughout all folds\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    #get non-hot-encoded columns\n",
    "    non_hot_cols = [col for col in X_train.columns if not col.startswith('Name_')]\n",
    "\n",
    "    # standardize the features\n",
    "    #new data will have mean of 0 and std. dev. of 1\n",
    "    #this help model more correctly handle different data of various magnitudes\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "    # Scale only the nont-hot-encoded features\n",
    "    X_train_scaled[non_hot_cols] = scaler.fit_transform(X_train[non_hot_cols])\n",
    "    X_test_scaled[non_hot_cols] = scaler.transform(X_test[non_hot_cols])\n",
    "\n",
    "    #initialize,train model\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    #make predictions\n",
    "    predictions = model.predict(X_test_scaled)\n",
    "    print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f09e3a6-6b87-4e68-be73-aa0ab99e0cf9",
   "metadata": {},
   "source": [
    "\\******************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************\n",
    "\n",
    "In this next section we will attempt to find hyperparamaters that increase the accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33bd1d97-b70f-40e7-af0e-c7e8916f1760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5846176d-0d5d-4ce5-9073-9811d10b8c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for 'var_smoothing'\n",
    "#create different values for the var_smoothing paramater\n",
    "#These values will range from very small 1 x 10^-10 to larger 1 x 10^-1\n",
    "param_grid = {'var_smoothing': [1e-10, 1e-09, 1e-08, 1e-07, 1e-06, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1e70ebf-824a-4590-ac7c-1b466e86ef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3aa56af-7fc4-4bec-a71c-5435ac431c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GaussianNB(),\n",
       "             param_grid={'var_smoothing': [1e-10, 1e-09, 1e-08, 1e-07, 1e-06,\n",
       "                                           1e-05, 0.0001, 0.001, 0.01, 0.1]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize GridSearchCV\n",
    "#GridSearchCV looks through the afforementioned hyper-paramaters\n",
    "#to find the most optimal one for the model\n",
    "grid_search = GridSearchCV(estimator=gnb, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Perform the grid search on scaled data\n",
    "grid_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fe793f1-3ee9-40d4-b1c0-63f93aa63ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'var_smoothing': 0.001}\n",
      "Best score: 0.4927942827973456\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.86      0.62     46853\n",
      "           1       0.53      0.14      0.23     51097\n",
      "\n",
      "    accuracy                           0.49     97950\n",
      "   macro avg       0.51      0.50      0.42     97950\n",
      "weighted avg       0.51      0.49      0.41     97950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print best paramater,score\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n",
    "\n",
    "# Test best model on test set\n",
    "best_model = grid_search.best_estimator_\n",
    "predictions = best_model.predict(X_test_scaled)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb5a314-2a2b-4a70-9811-76500ab00eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
