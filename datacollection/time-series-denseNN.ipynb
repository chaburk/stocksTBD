{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c816fd52-d33a-45d8-8bbe-51fd5e7d3b2c",
   "metadata": {},
   "source": [
    "This file contains the most promising models - time series based neural networks\n",
    "There are a plethora included in the file with variations in the way they were trained. The best performing model is exported for use on the website\n",
    "\n",
    "Author: Sean Brady\n",
    "Created: Mar 10, 2024\n",
    "Updated: officially Mar 24, 2024, but has been updated locally ever since as we have chosen to focus on these models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93bac465-a686-40c9-9fbd-d221257dd7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6261a7d1-6fae-4d8d-b3e9-688c491eeba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_symbols(path): #func to read in the symbols from companies.txt\n",
    "    with open(path,'r') as file: #open file\n",
    "        return file.read().splitlines() #return array of company symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ce3baa-0c91-4e1f-802c-2359aa077d57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MMM', 'AOS', 'ABT', 'ABBV', 'ACN', 'ADBE', 'AMD', 'AES', 'AFL', 'A', 'APD', 'ABNB', 'AKAM', 'ALB', 'ARE', 'ALGN', 'ALLE', 'LNT', 'ALL', 'GOOGL', 'GOOG', 'MO', 'AMZN', 'AMCR', 'AEE', 'AAL', 'AEP', 'AXP', 'AIG', 'AMT', 'AWK', 'AMP', 'AME', 'AMGN', 'APH', 'ADI', 'ANSS', 'AON', 'APA', 'AAPL', 'AMAT', 'APTV', 'ACGL', 'ADM', 'ANET', 'AJG', 'AIZ', 'T', 'ATO', 'ADSK', 'ADP', 'AZO', 'AVB', 'AVY', 'AXON', 'BKR', 'BALL', 'BAC', 'BK', 'BBWI', 'BAX', 'BDX', 'BRK.B', 'BBY', 'BIO', 'TECH', 'BIIB', 'BLK', 'BX', 'BA', 'BKNG', 'BWA', 'BXP', 'BSX', 'BMY', 'AVGO', 'BR', 'BRO', 'BF.B', 'BLDR', 'BG', 'CDNS', 'CZR', 'CPT', 'CPB', 'COF', 'CAH', 'KMX', 'CCL', 'CARR', 'CTLT', 'CAT', 'CBOE', 'CBRE', 'CDW', 'CE', 'COR', 'CNC', 'CNP', 'CF', 'CHRW', 'CRL', 'SCHW', 'CHTR', 'CVX', 'CMG', 'CB', 'CHD', 'CI', 'CINF', 'CTAS', 'CSCO', 'C', 'CFG', 'CLX', 'CME', 'CMS', 'KO', 'CTSH', 'CL', 'CMCSA', 'CMA', 'CAG', 'COP', 'ED', 'STZ', 'CEG', 'COO', 'CPRT', 'GLW', 'CPAY', 'CTVA', 'CSGP', 'COST', 'CTRA', 'CCI', 'CSX', 'CMI', 'CVS', 'DHR', 'DRI', 'DVA', 'DAY', 'DECK', 'DE', 'DAL', 'DVN', 'DXCM', 'FANG', 'DLR', 'DFS', 'DG', 'DLTR', 'D', 'DPZ', 'DOV', 'DOW', 'DHI', 'DTE', 'DUK', 'DD', 'EMN', 'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'ELV', 'LLY', 'EMR', 'ENPH', 'ETR', 'EOG', 'EPAM', 'EQT', 'EFX', 'EQIX', 'EQR', 'ESS', 'EL', 'ETSY', 'EG', 'EVRG', 'ES', 'EXC', 'EXPE', 'EXPD', 'EXR', 'XOM', 'FFIV', 'FDS', 'FICO', 'FAST', 'FRT', 'FDX', 'FIS', 'FITB', 'FSLR', 'FE', 'FI', 'FMC', 'F', 'FTNT', 'FTV', 'FOXA', 'FOX', 'BEN', 'FCX', 'GRMN', 'IT', 'GE', 'GEHC', 'GEV', 'GEN', 'GNRC', 'GD', 'GIS', 'GM', 'GPC', 'GILD', 'GPN', 'GL', 'GS', 'HAL', 'HIG', 'HAS', 'HCA', 'DOC', 'HSIC', 'HSY', 'HES', 'HPE', 'HLT', 'HOLX', 'HD', 'HON', 'HRL', 'HST', 'HWM', 'HPQ', 'HUBB', 'HUM', 'HBAN', 'HII', 'IBM', 'IEX', 'IDXX', 'ITW', 'ILMN', 'INCY', 'IR', 'PODD', 'INTC', 'ICE', 'IFF', 'IP', 'IPG', 'INTU', 'ISRG', 'IVZ', 'INVH', 'IQV', 'IRM', 'JBHT', 'JBL', 'JKHY', 'J', 'JNJ', 'JCI', 'JPM', 'JNPR', 'K', 'KVUE', 'KDP', 'KEY', 'KEYS', 'KMB', 'KIM', 'KMI', 'KLAC', 'KHC', 'KR', 'LHX', 'LH', 'LRCX', 'LW', 'LVS', 'LDOS', 'LEN', 'LIN', 'LYV', 'LKQ', 'LMT', 'L', 'LOW', 'LULU', 'LYB', 'MTB', 'MRO', 'MPC', 'MKTX', 'MAR', 'MMC', 'MLM', 'MAS', 'MA', 'MTCH', 'MKC', 'MCD', 'MCK', 'MDT', 'MRK', 'META', 'MET', 'MTD', 'MGM', 'MCHP', 'MU', 'MSFT', 'MAA', 'MRNA', 'MHK', 'MOH', 'TAP', 'MDLZ', 'MPWR', 'MNST', 'MCO', 'MS', 'MOS', 'MSI', 'MSCI', 'NDAQ', 'NTAP', 'NFLX', 'NEM', 'NWSA', 'NWS', 'NEE', 'NKE', 'NI', 'NDSN', 'NSC', 'NTRS', 'NOC', 'NCLH', 'NRG', 'NUE', 'NVDA', 'NVR', 'NXPI', 'ORLY', 'OXY', 'ODFL', 'OMC', 'ON', 'OKE', 'ORCL', 'OTIS', 'PCAR', 'PKG', 'PANW', 'PARA', 'PH', 'PAYX', 'PAYC', 'PYPL', 'PNR', 'PEP', 'PFE', 'PCG', 'PM', 'PSX', 'PNW', 'PXD', 'PNC', 'POOL', 'PPG', 'PPL', 'PFG', 'PG', 'PGR', 'PLD', 'PRU', 'PEG', 'PTC', 'PSA', 'PHM', 'QRVO', 'PWR', 'QCOM', 'DGX', 'RL', 'RJF', 'RTX', 'O', 'REG', 'REGN', 'RF', 'RSG', 'RMD', 'RVTY', 'RHI', 'ROK', 'ROL', 'ROP', 'ROST', 'RCL', 'SPGI', 'CRM', 'SBAC', 'SLB', 'STX', 'SRE', 'NOW', 'SHW', 'SPG', 'SWKS', 'SJM', 'SNA', 'SOLV', 'SO', 'LUV', 'SWK', 'SBUX', 'STT', 'STLD', 'STE', 'SYK', 'SMCI', 'SYF', 'SNPS', 'SYY', 'TMUS', 'TROW', 'TTWO', 'TPR', 'TRGP', 'TGT', 'TEL', 'TDY', 'TFX', 'TER', 'TSLA', 'TXN', 'TXT', 'TMO', 'TJX', 'TSCO', 'TT', 'TDG', 'TRV', 'TRMB', 'TFC', 'TYL', 'TSN', 'USB', 'UBER', 'UDR', 'ULTA', 'UNP', 'UAL', 'UPS', 'URI', 'UNH', 'UHS', 'VLO', 'VTR', 'VLTO', 'VRSN', 'VRSK', 'VZ', 'VRTX', 'VTRS', 'VICI', 'V', 'VMC', 'WRB', 'WAB', 'WBA', 'WMT', 'DIS', 'WBD', 'WM', 'WAT', 'WEC', 'WFC', 'WELL', 'WST', 'WDC', 'WRK', 'WY', 'WMB', 'WTW', 'GWW', 'WYNN', 'XEL', 'XYL', 'YUM', 'ZBRA', 'ZBH', 'ZTS']\n"
     ]
    }
   ],
   "source": [
    "#get array of symbols\n",
    "symbols_path = 'companies.txt'\n",
    "symbols = read_symbols(symbols_path)\n",
    "print(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55b451d1-1c89-4d1e-8b1e-d9fcb6d461e6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMM:\n",
      "\n",
      "(5974, 26)\n",
      "\n",
      "\n",
      "AOS:\n",
      "\n",
      "(4682, 26)\n",
      "\n",
      "\n",
      "ABT:\n",
      "\n",
      "(5974, 26)\n",
      "\n",
      "\n",
      "ABBV:\n",
      "\n",
      "(2705, 26)\n",
      "\n",
      "\n",
      "ACN:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "ADBE:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "AMD:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "AES:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "AFL:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "A:\n",
      "\n",
      "(6000, 26)\n",
      "\n",
      "\n",
      "APD:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "AKAM:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "ALB:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "ARE:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "ALGN:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "ALLE:\n",
      "\n",
      "(2504, 26)\n",
      "\n",
      "\n",
      "LNT:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "ALL:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "GOOGL:\n",
      "\n",
      "(4842, 26)\n",
      "\n",
      "\n",
      "GOOG:\n",
      "\n",
      "(2427, 26)\n",
      "\n",
      "\n",
      "MO:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "AMZN:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "AMCR:\n",
      "\n",
      "(1116, 26)\n",
      "\n",
      "\n",
      "AEE:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "AAL:\n",
      "\n",
      "(2500, 26)\n",
      "\n",
      "\n",
      "AEP:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "AXP:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "AIG:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "AMT:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "AWK:\n",
      "\n",
      "(3919, 26)\n",
      "\n",
      "\n",
      "AMP:\n",
      "\n",
      "(4572, 26)\n",
      "\n",
      "\n",
      "AME:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "AMGN:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "APH:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "ADI:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "ANSS:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "AON:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "APA:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "AAPL:\n",
      "\n",
      "(6006, 26)\n",
      "\n",
      "\n",
      "AMAT:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "APTV:\n",
      "\n",
      "(3015, 26)\n",
      "\n",
      "\n",
      "ADM:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "ANET:\n",
      "\n",
      "(2378, 26)\n",
      "\n",
      "\n",
      "AJG:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "AIZ:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "T:\n",
      "\n",
      "(6006, 26)\n",
      "\n",
      "\n",
      "ATO:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "ADSK:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "ADP:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "AZO:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "AVB:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "AVY:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "BKR:\n",
      "\n",
      "(1602, 26)\n",
      "\n",
      "\n",
      "BAC:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "BK:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "BBWI:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "BAX:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "BDX:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "BBY:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "BIO:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "TECH:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "BIIB:\n",
      "\n",
      "(6000, 26)\n",
      "\n",
      "\n",
      "BLK:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "BA:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "BKNG:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "BWA:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "BXP:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "BSX:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "BMY:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "AVGO:\n",
      "\n",
      "(3594, 26)\n",
      "\n",
      "\n",
      "BR:\n",
      "\n",
      "(4192, 26)\n",
      "\n",
      "\n",
      "BRO:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "CDNS:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "CZR:\n",
      "\n",
      "(2302, 26)\n",
      "\n",
      "\n",
      "CPB:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "COF:\n",
      "\n",
      "(6000, 26)\n",
      "\n",
      "\n",
      "CAH:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "KMX:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "CCL:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "CARR:\n",
      "\n",
      "(910, 26)\n",
      "\n",
      "\n",
      "CTLT:\n",
      "\n",
      "(2340, 26)\n",
      "\n",
      "\n",
      "CAT:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "CBOE:\n",
      "\n",
      "(3379, 26)\n",
      "\n",
      "\n",
      "CBRE:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "CDW:\n",
      "\n",
      "(2615, 26)\n",
      "\n",
      "\n",
      "CE:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "CNC:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "CNP:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "CF:\n",
      "\n",
      "(4597, 26)\n",
      "\n",
      "\n",
      "CHRW:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "CRL:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "SCHW:\n",
      "\n",
      "(6005, 26)\n",
      "\n",
      "\n",
      "CHTR:\n",
      "\n",
      "(3487, 26)\n",
      "\n",
      "\n",
      "CVX:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "CMG:\n",
      "\n",
      "(4482, 26)\n",
      "\n",
      "\n",
      "CB:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "CHD:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "CI:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "CINF:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "CTAS:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "CSCO:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "C:\n",
      "\n",
      "(5999, 26)\n",
      "\n",
      "\n",
      "CFG:\n",
      "\n",
      "(2302, 26)\n",
      "\n",
      "\n",
      "CLX:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "CME:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "CMS:\n",
      "\n",
      "(5998, 26)\n",
      "\n",
      "\n",
      "KO:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "CTSH:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "CL:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "CMCSA:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "CMA:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "CAG:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "COP:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "ED:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "STZ:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "COO:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "CPRT:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "GLW:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "CTVA:\n",
      "\n",
      "(1122, 26)\n",
      "\n",
      "\n",
      "COST:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "CTRA:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "CCI:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "CSX:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "CMI:\n",
      "\n",
      "(5995, 26)\n",
      "\n",
      "\n",
      "CVS:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "DHR:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "DRI:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "DVA:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "DE:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "DAL:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "DVN:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "DXCM:\n",
      "\n",
      "(4679, 26)\n",
      "\n",
      "\n",
      "FANG:\n",
      "\n",
      "(2790, 26)\n",
      "\n",
      "\n",
      "DLR:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "DFS:\n",
      "\n",
      "(4134, 26)\n",
      "\n",
      "\n",
      "DG:\n",
      "\n",
      "(3524, 26)\n",
      "\n",
      "\n",
      "DLTR:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "D:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "DPZ:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "DOV:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "DOW:\n",
      "\n",
      "(1164, 26)\n",
      "\n",
      "\n",
      "DHI:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "DTE:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "DUK:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "DD:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "EMN:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "ETN:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "EBAY:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "ECL:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "EIX:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "EW:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "EA:\n",
      "\n",
      "(2994, 26)\n",
      "\n",
      "\n",
      "LLY:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "EMR:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "ENPH:\n",
      "\n",
      "(2926, 26)\n",
      "\n",
      "\n",
      "ETR:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "EOG:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "EFX:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "EQIX:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "EQR:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "ESS:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "EL:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "ETSY:\n",
      "\n",
      "(2162, 26)\n",
      "\n",
      "\n",
      "EVRG:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "ES:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "EXC:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "EXPE:\n",
      "\n",
      "(4612, 26)\n",
      "\n",
      "\n",
      "EXPD:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "EXR:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "XOM:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "FFIV:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "FAST:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "FRT:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "FDX:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "FIS:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "FITB:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "FE:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "FMC:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "F:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "FTNT:\n",
      "\n",
      "(3521, 26)\n",
      "\n",
      "\n",
      "FTV:\n",
      "\n",
      "(1854, 26)\n",
      "\n",
      "\n",
      "FOXA:\n",
      "\n",
      "(1179, 26)\n",
      "\n",
      "\n",
      "FOX:\n",
      "\n",
      "(1179, 26)\n",
      "\n",
      "\n",
      "BEN:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "FCX:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "GRMN:\n",
      "\n",
      "(5764, 26)\n",
      "\n",
      "\n",
      "IT:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "GE:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "GNRC:\n",
      "\n",
      "(3464, 26)\n",
      "\n",
      "\n",
      "GD:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "GIS:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "GM:\n",
      "\n",
      "(3270, 26)\n",
      "\n",
      "\n",
      "GPC:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "GILD:\n",
      "\n",
      "(5999, 26)\n",
      "\n",
      "\n",
      "GPN:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "GL:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "GS:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "HAL:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "HIG:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "HAS:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "HCA:\n",
      "\n",
      "(3190, 26)\n",
      "\n",
      "\n",
      "HSIC:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "HSY:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "HES:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "HPE:\n",
      "\n",
      "(2022, 26)\n",
      "\n",
      "\n",
      "HLT:\n",
      "\n",
      "(2498, 26)\n",
      "\n",
      "\n",
      "HOLX:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "HD:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "HON:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "HRL:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "HST:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "HWM:\n",
      "\n",
      "(1771, 26)\n",
      "\n",
      "\n",
      "HPQ:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "HUM:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "HBAN:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "HII:\n",
      "\n",
      "(3175, 26)\n",
      "\n",
      "\n",
      "IBM:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "IEX:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "IDXX:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "ITW:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "ILMN:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "INCY:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "IR:\n",
      "\n",
      "(1639, 26)\n",
      "\n",
      "\n",
      "INTC:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "ICE:\n",
      "\n",
      "(4529, 26)\n",
      "\n",
      "\n",
      "IFF:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "IP:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "IPG:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "INTU:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "ISRG:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "IVZ:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "IQV:\n",
      "\n",
      "(2649, 26)\n",
      "\n",
      "\n",
      "IRM:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "JBHT:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "JKHY:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "J:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "JNJ:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "JCI:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "JPM:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "JNPR:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "K:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "KEY:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "KEYS:\n",
      "\n",
      "(2272, 26)\n",
      "\n",
      "\n",
      "KMB:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "KIM:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "KMI:\n",
      "\n",
      "(3208, 26)\n",
      "\n",
      "\n",
      "KLAC:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "KHC:\n",
      "\n",
      "(2107, 26)\n",
      "\n",
      "\n",
      "KR:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "LHX:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "LH:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "LRCX:\n",
      "\n",
      "(5997, 26)\n",
      "\n",
      "\n",
      "LW:\n",
      "\n",
      "(1763, 26)\n",
      "\n",
      "\n",
      "LVS:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "LDOS:\n",
      "\n",
      "(4301, 26)\n",
      "\n",
      "\n",
      "LEN:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "LIN:\n",
      "\n",
      "(1267, 26)\n",
      "\n",
      "\n",
      "LYV:\n",
      "\n",
      "(4509, 26)\n",
      "\n",
      "\n",
      "LKQ:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "LMT:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "L:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "LOW:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "LYB:\n",
      "\n",
      "(3412, 26)\n",
      "\n",
      "\n",
      "MTB:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "MRO:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "MPC:\n",
      "\n",
      "(3112, 26)\n",
      "\n",
      "\n",
      "MKTX:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "MAR:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "MMC:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "MLM:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "MAS:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "MA:\n",
      "\n",
      "(4399, 26)\n",
      "\n",
      "\n",
      "MTCH:\n",
      "\n",
      "(2010, 26)\n",
      "\n",
      "\n",
      "MKC:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "MCD:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "MCK:\n",
      "\n",
      "(6000, 26)\n",
      "\n",
      "\n",
      "MDT:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "MRK:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "MET:\n",
      "\n",
      "(5939, 26)\n",
      "\n",
      "\n",
      "MTD:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "MGM:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "MCHP:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "MU:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "MSFT:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "MAA:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "MRNA:\n",
      "\n",
      "(1243, 26)\n",
      "\n",
      "\n",
      "MHK:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "TAP:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "MDLZ:\n",
      "\n",
      "(4310, 26)\n",
      "\n",
      "\n",
      "MPWR:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "MNST:\n",
      "\n",
      "(2982, 26)\n",
      "\n",
      "\n",
      "MCO:\n",
      "\n",
      "(5813, 26)\n",
      "\n",
      "\n",
      "MS:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "MOS:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "MSI:\n",
      "\n",
      "(4713, 26)\n",
      "\n",
      "\n",
      "MSCI:\n",
      "\n",
      "(4027, 26)\n",
      "\n",
      "\n",
      "NDAQ:\n",
      "\n",
      "(4711, 26)\n",
      "\n",
      "\n",
      "NTAP:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "NFLX:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "NEM:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "NWSA:\n",
      "\n",
      "(2618, 26)\n",
      "\n",
      "\n",
      "NWS:\n",
      "\n",
      "(2618, 26)\n",
      "\n",
      "\n",
      "NEE:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "NKE:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "NI:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "NSC:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "NTRS:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "NOC:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "NCLH:\n",
      "\n",
      "(2725, 26)\n",
      "\n",
      "\n",
      "NRG:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "NUE:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "NVDA:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "NVR:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "NXPI:\n",
      "\n",
      "(3342, 26)\n",
      "\n",
      "\n",
      "ORLY:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "OXY:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "ODFL:\n",
      "\n",
      "(4713, 26)\n",
      "\n",
      "\n",
      "OMC:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "OKE:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "ORCL:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "OTIS:\n",
      "\n",
      "(910, 26)\n",
      "\n",
      "\n",
      "PCAR:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "PKG:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "PH:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "PAYX:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "PAYC:\n",
      "\n",
      "(2414, 26)\n",
      "\n",
      "\n",
      "PYPL:\n",
      "\n",
      "(2107, 26)\n",
      "\n",
      "\n",
      "PNR:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "PEP:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "PFE:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "PM:\n",
      "\n",
      "(3944, 26)\n",
      "\n",
      "\n",
      "PSX:\n",
      "\n",
      "(2903, 26)\n",
      "\n",
      "\n",
      "PNW:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "PXD:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "PNC:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "POOL:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "PPG:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "PPL:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "PFG:\n",
      "\n",
      "(5552, 26)\n",
      "\n",
      "\n",
      "PG:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "PGR:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "PLD:\n",
      "\n",
      "(5997, 26)\n",
      "\n",
      "\n",
      "PRU:\n",
      "\n",
      "(5514, 26)\n",
      "\n",
      "\n",
      "PEG:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "PTC:\n",
      "\n",
      "(2502, 26)\n",
      "\n",
      "\n",
      "PSA:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "PHM:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "QRVO:\n",
      "\n",
      "(2207, 26)\n",
      "\n",
      "\n",
      "PWR:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "QCOM:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "DGX:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "RL:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "RJF:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "RTX:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "O:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "REG:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "REGN:\n",
      "\n",
      "(4711, 26)\n",
      "\n",
      "\n",
      "RF:\n",
      "\n",
      "(5656, 26)\n",
      "\n",
      "\n",
      "RSG:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "RMD:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "RHI:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "ROK:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "ROL:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "ROP:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "ROST:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "RCL:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "SPGI:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "CRM:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "SBAC:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "SLB:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "STX:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "SRE:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "NOW:\n",
      "\n",
      "(2863, 26)\n",
      "\n",
      "\n",
      "SHW:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "SPG:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "SWKS:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "SJM:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "SNA:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "SO:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "LUV:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "SWK:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "SBUX:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "STT:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "STE:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "SYK:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "SYF:\n",
      "\n",
      "(2340, 26)\n",
      "\n",
      "\n",
      "SNPS:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "SYY:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "TMUS:\n",
      "\n",
      "(4174, 26)\n",
      "\n",
      "\n",
      "TROW:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "TTWO:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "TPR:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "TGT:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "TEL:\n",
      "\n",
      "(4134, 26)\n",
      "\n",
      "\n",
      "TDY:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "TFX:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "TER:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "TSLA:\n",
      "\n",
      "(3369, 26)\n",
      "\n",
      "\n",
      "TXN:\n",
      "\n",
      "(6000, 26)\n",
      "\n",
      "\n",
      "TXT:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "TMO:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "TJX:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "TSCO:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "TT:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "TDG:\n",
      "\n",
      "(4449, 26)\n",
      "\n",
      "\n",
      "TRV:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "TRMB:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "TFC:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "TYL:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "TSN:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "USB:\n",
      "\n",
      "(5999, 26)\n",
      "\n",
      "\n",
      "UDR:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "ULTA:\n",
      "\n",
      "(4042, 26)\n",
      "\n",
      "\n",
      "UNP:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "UAL:\n",
      "\n",
      "(4483, 26)\n",
      "\n",
      "\n",
      "UPS:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "URI:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "UNH:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "UHS:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "VLO:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "VTR:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "VRSN:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "VRSK:\n",
      "\n",
      "(3551, 26)\n",
      "\n",
      "\n",
      "VZ:\n",
      "\n",
      "(6002, 26)\n",
      "\n",
      "\n",
      "VRTX:\n",
      "\n",
      "(4710, 26)\n",
      "\n",
      "\n",
      "VTRS:\n",
      "\n",
      "(766, 26)\n",
      "\n",
      "\n",
      "V:\n",
      "\n",
      "(3943, 26)\n",
      "\n",
      "\n",
      "VMC:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "WRB:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "WAB:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "WBA:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "WMT:\n",
      "\n",
      "(6003, 26)\n",
      "\n",
      "\n",
      "DIS:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "WM:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "WAT:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "WEC:\n",
      "\n",
      "(5999, 26)\n",
      "\n",
      "\n",
      "WFC:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "WELL:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "WST:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "WDC:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "WRK:\n",
      "\n",
      "(2107, 26)\n",
      "\n",
      "\n",
      "WY:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "WMB:\n",
      "\n",
      "(6001, 26)\n",
      "\n",
      "\n",
      "GWW:\n",
      "\n",
      "(6004, 26)\n",
      "\n",
      "\n",
      "WYNN:\n",
      "\n",
      "(4712, 26)\n",
      "\n",
      "\n",
      "XEL:\n",
      "\n",
      "(5844, 26)\n",
      "\n",
      "\n",
      "XYL:\n",
      "\n",
      "(3027, 26)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = {} #dictionary to hold data\n",
    "for symbol in symbols:\n",
    "    data_path = f\"company-data/{symbol}-data.csv\" #path to company specific data\n",
    "    try:\n",
    "        df_ind = pd.read_csv(data_path)\n",
    "        df_ind['target'] = df_ind['Close'].shift(-1) #create target variable (next days close variable)\n",
    "        df_ind = df_ind.dropna() #drop NaN values\n",
    "        print(f'{symbol}:\\n')\n",
    "        print(df_ind.shape)\n",
    "        print('\\n')\n",
    "        data[symbol] = df_ind\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dd797e5-4365-4376-811b-a0fe35221141",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1792682, 25)\n",
      "(448428, 25)\n",
      "                      Open     High      Low    Close        Volume  \\\n",
      "Ticker Date                                                           \n",
      "MMM    2000-03-29  31.1995  31.1995  30.6183  30.7074  4.594542e+06   \n",
      "       2000-03-30  30.8634  32.1536  30.7074  31.1995  4.419489e+06   \n",
      "       2000-03-31  31.6586  32.2485  31.0396  31.0396  3.443945e+06   \n",
      "       2000-04-03  31.3332  32.3754  31.2828  32.1971  4.021207e+06   \n",
      "       2000-04-04  32.2485  33.4970  31.3767  32.5846  6.779498e+06   \n",
      "       2000-04-05  32.5555  33.2248  31.9134  31.9328  4.027360e+06   \n",
      "       2000-04-06  31.9976  33.2461  31.9976  32.7125  2.459723e+06   \n",
      "       2000-04-07  32.8045  33.3400  32.1119  32.1119  3.502765e+06   \n",
      "       2000-04-10  32.1971  33.2461  32.1536  32.5110  2.955758e+06   \n",
      "       2000-04-11  32.5110  34.0123  32.5110  33.4485  4.134358e+06   \n",
      "\n",
      "                   daily_returns  2_day_ma   3_day_ma  5_day_ma   7_day_ma  \\\n",
      "Ticker Date                                                                  \n",
      "MMM    2000-03-29      -0.014304  30.93020  30.973800  31.04858  31.011586   \n",
      "       2000-03-30       0.016025  30.95345  31.019967  31.19212  30.941571   \n",
      "       2000-03-31      -0.005125  31.11955  30.982167  31.03210  31.068857   \n",
      "       2000-04-03       0.037291  31.61835  31.478733  31.25932  31.313900   \n",
      "       2000-04-04       0.012035  32.39085  31.940433  31.54564  31.420314   \n",
      "       2000-04-05      -0.020003  32.25870  32.238167  31.79072  31.544857   \n",
      "       2000-04-06       0.024417  32.32265  32.409967  32.09332  31.767643   \n",
      "       2000-04-07      -0.018360  32.41220  32.252400  32.30778  31.968286   \n",
      "       2000-04-10       0.012428  32.31145  32.445133  32.37056  32.155643   \n",
      "       2000-04-11       0.028836  32.97975  32.690467  32.54334  32.499771   \n",
      "\n",
      "                   ...  5_day_volatility  7_day_volatility  10_day_volatility  \\\n",
      "Ticker Date        ...                                                          \n",
      "MMM    2000-03-29  ...          0.026645          0.033755           0.036047   \n",
      "       2000-03-30  ...          0.027065          0.030321           0.029526   \n",
      "       2000-03-31  ...          0.015577          0.022607           0.028235   \n",
      "       2000-04-03  ...          0.020097          0.025841           0.030145   \n",
      "       2000-04-04  ...          0.020008          0.020595           0.027894   \n",
      "       2000-04-05  ...          0.021766          0.019634           0.023038   \n",
      "       2000-04-06  ...          0.022833          0.021046           0.023740   \n",
      "       2000-04-07  ...          0.025586          0.021779           0.020956   \n",
      "       2000-04-10  ...          0.020066          0.021561           0.019135   \n",
      "       2000-04-11  ...          0.023292          0.022419           0.020565   \n",
      "\n",
      "                   20_day_volatility  30_day_volatility  2_day_support  \\\n",
      "Ticker Date                                                              \n",
      "MMM    2000-03-29           0.032579           0.030741        30.7074   \n",
      "       2000-03-30           0.032737           0.030820        30.7074   \n",
      "       2000-03-31           0.031522           0.030727        31.0396   \n",
      "       2000-04-03           0.031973           0.031121        31.0396   \n",
      "       2000-04-04           0.029368           0.030760        32.1971   \n",
      "       2000-04-05           0.029896           0.030486        31.9328   \n",
      "       2000-04-06           0.029563           0.030677        31.9328   \n",
      "       2000-04-07           0.030127           0.028865        32.1119   \n",
      "       2000-04-10           0.028874           0.028907        32.1119   \n",
      "       2000-04-11           0.029172           0.029122        32.5110   \n",
      "\n",
      "                   2_day_resistance  2_day_avg_volume  2_day_stoch_k   target  \n",
      "Ticker Date                                                                    \n",
      "MMM    2000-03-29           31.1530      4.011402e+06       7.985302  31.1995  \n",
      "       2000-03-30           31.1995      4.507016e+06      37.855794  31.0396  \n",
      "       2000-03-31           31.1995      3.931717e+06      21.556031  32.1971  \n",
      "       2000-04-03           32.1971      3.732576e+06      86.652193  32.5846  \n",
      "       2000-04-04           32.5846      5.400353e+06      58.793244  31.9328  \n",
      "       2000-04-05           32.5846      5.403429e+06      26.227421  32.7125  \n",
      "       2000-04-06           32.7125      3.243542e+06      59.960981  32.1119  \n",
      "       2000-04-07           32.7125      2.981244e+06       8.514601  32.5110  \n",
      "       2000-04-10           32.5110      3.229261e+06      32.497354  33.4485  \n",
      "       2000-04-11           33.4485      3.545058e+06      69.666972  33.1619  \n",
      "\n",
      "[10 rows x 25 columns]\n",
      "                      Open     High      Low    Close        Volume  \\\n",
      "Ticker Date                                                           \n",
      "MMM    2019-04-02  185.332  185.738  184.247  185.235  1.618739e+06   \n",
      "       2019-04-03  185.941  186.822  185.506  186.097  1.797920e+06   \n",
      "       2019-04-04  186.435  187.965  185.409  187.781  1.787580e+06   \n",
      "       2019-04-05  188.509  189.031  187.570  188.082  1.593088e+06   \n",
      "       2019-04-08  187.491  187.821  186.407  187.771  1.294128e+06   \n",
      "       2019-04-09  187.230  187.366  184.954  185.079  1.736644e+06   \n",
      "       2019-04-10  185.486  185.747  183.996  185.215  1.544629e+06   \n",
      "       2019-04-11  184.867  186.619  184.644  186.465  1.352770e+06   \n",
      "       2019-04-12  187.850  189.902  187.162  189.875  2.201551e+06   \n",
      "       2019-04-15  189.331  189.815  187.657  188.459  1.698697e+06   \n",
      "\n",
      "                   daily_returns  2_day_ma    3_day_ma  5_day_ma    7_day_ma  \\\n",
      "Ticker Date                                                                    \n",
      "MMM    2019-04-02       0.000940  185.1480  183.911333  182.9452  181.876571   \n",
      "       2019-04-03       0.004654  185.6660  185.464333  183.9854  183.112286   \n",
      "       2019-04-04       0.009049  186.9390  186.371000  185.1224  184.086286   \n",
      "       2019-04-05       0.001603  187.9315  187.320000  186.4512  185.112857   \n",
      "       2019-04-08      -0.001654  187.9265  187.878000  186.9932  185.923571   \n",
      "       2019-04-09      -0.014337  186.4250  186.977333  186.9620  186.443714   \n",
      "       2019-04-10       0.000735  185.1470  186.021667  186.7856  186.465714   \n",
      "       2019-04-11       0.006749  185.8400  185.586333  186.5224  186.641429   \n",
      "       2019-04-12       0.018288  188.1700  187.185000  186.8810  187.181143   \n",
      "       2019-04-15      -0.007458  189.1670  188.266333  187.0186  187.278000   \n",
      "\n",
      "                   ...  5_day_volatility  7_day_volatility  10_day_volatility  \\\n",
      "Ticker Date        ...                                                          \n",
      "MMM    2019-04-02  ...          0.009298          0.010892           0.012837   \n",
      "       2019-04-03  ...          0.008873          0.009501           0.012737   \n",
      "       2019-04-04  ...          0.009001          0.007768           0.012861   \n",
      "       2019-04-05  ...          0.007801          0.007560           0.009066   \n",
      "       2019-04-08  ...          0.004097          0.008007           0.008387   \n",
      "       2019-04-09  ...          0.008864          0.010453           0.008937   \n",
      "       2019-04-10  ...          0.008500          0.007253           0.008908   \n",
      "       2019-04-11  ...          0.007864          0.007680           0.008914   \n",
      "       2019-04-12  ...          0.011934          0.010113           0.009905   \n",
      "       2019-04-15  ...          0.012634          0.010366           0.008925   \n",
      "\n",
      "                   20_day_volatility  30_day_volatility  2_day_support  \\\n",
      "Ticker Date                                                              \n",
      "MMM    2019-04-02           0.011988           0.010555        185.061   \n",
      "       2019-04-03           0.011880           0.010580        185.235   \n",
      "       2019-04-04           0.011252           0.010650        186.097   \n",
      "       2019-04-05           0.011256           0.010582        187.781   \n",
      "       2019-04-08           0.009270           0.010553        187.771   \n",
      "       2019-04-09           0.009805           0.010849        185.079   \n",
      "       2019-04-10           0.009788           0.010842        185.079   \n",
      "       2019-04-11           0.009851           0.010778        185.215   \n",
      "       2019-04-12           0.010540           0.011228        186.465   \n",
      "       2019-04-15           0.010762           0.011321        188.459   \n",
      "\n",
      "                   2_day_resistance  2_day_avg_volume  2_day_stoch_k   target  \n",
      "Ticker Date                                                                    \n",
      "MMM    2019-04-02           185.235      2.291518e+06      83.244504  186.097  \n",
      "       2019-04-03           186.097      1.708329e+06      71.844660  187.781  \n",
      "       2019-04-04           187.781      1.792750e+06      92.801252  188.082  \n",
      "       2019-04-05           188.082      1.690334e+06      73.799006  187.771  \n",
      "       2019-04-08           188.082      1.443608e+06      51.981707  185.079  \n",
      "       2019-04-09           187.771      1.515386e+06       4.359958  185.215  \n",
      "       2019-04-10           185.215      1.640637e+06      36.172107  186.465  \n",
      "       2019-04-11           186.465      1.448699e+06      94.128860  189.875  \n",
      "       2019-04-12           189.875      1.777160e+06      99.486497  188.459  \n",
      "       2019-04-15           189.875      1.950124e+06      47.335766  189.390  \n",
      "\n",
      "[10 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "#arrays to hold the training and testing portions of each dataframe\n",
    "train_frames = []\n",
    "test_frames = []\n",
    "\n",
    "for ticker, df in data.items(): #iterate over dictionary\n",
    "    df['Ticker'] = ticker #add ticker as column\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.set_index(['Ticker','Date']) #create multi-index\n",
    "    #df = df.set_index(['Ticker', df.index]) #create multiindex [Ticker,Date]\n",
    "    #df[ticker] = df #set as new dataframe\n",
    "    \n",
    "    #split at 80% mark\n",
    "    split = int(len(df)*0.8)\n",
    "    \n",
    "    train_df = df.iloc[:split]\n",
    "    test_df = df.iloc[split:]\n",
    "    \n",
    "    train_frames.append(train_df)\n",
    "    test_frames.append(test_df)\n",
    "    #train_frames.append(df.iloc[:split])\n",
    "    #test_frames.append(df.iloc[split:])\n",
    "    \n",
    "train_df = pd.concat(train_frames)#, ignore_index=True)\n",
    "test_df = pd.concat(test_frames)#, ignore_index=True)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "print(train_df.head(10))\n",
    "print(test_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26d62bc9-0fd4-46af-b73a-6747a87b2c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ticker', 'Date']\n",
      "['Ticker', 'Date']\n"
     ]
    }
   ],
   "source": [
    "#make sure multi-indexing is working:\n",
    "print(train_df.index.names)\n",
    "print(test_df.index.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7be4bb49-3e3d-495c-a52a-d71b08f3d330",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'daily_returns', '2_day_ma', '3_day_ma', '5_day_ma', '7_day_ma', '10_day_ma', '20_day_ma', '30_day_ma', '2_day_volatility', '3_day_volatility', '5_day_volatility', '7_day_volatility', '10_day_volatility', '20_day_volatility', '30_day_volatility', '2_day_support', '2_day_resistance', '2_day_avg_volume', '2_day_stoch_k', 'target', 'Ticker']\n"
     ]
    }
   ],
   "source": [
    "#print(data['MMM'].head(10))\n",
    "print(list(data['MMM'].columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "204ed5c1-f39f-4559-be43-de1f085ee53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the data\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_train = scaler.fit_transform(train_df.drop(['target'], axis=1))\n",
    "scaled_test = scaler.transform(test_df.drop(['target'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b956314-ad32-4c1e-83a7-c938f3237ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(features, target, sequence_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(features) - sequence_length):#loop through to create sequence data\n",
    "        seq = features[i:i + sequence_length] #get a sequence of len sequence_length\n",
    "        X.append(seq) #append to X\n",
    "        target_value = target.iloc[i + sequence_length] #get target\n",
    "        y.append(target_value)#append to y\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be1227c5-4cdf-4d84-b028-565cd7ebe000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "def evaluate_model(predictions, y_true):\n",
    "    mae = mean_absolute_error(y_true, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, predictions))\n",
    "    print(f\"MAE: {mae}, RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81af5893-bdf5-4ab8-be50-2c425ef7167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train/test sequences\n",
    "sequence_length = 7 # holds the length of each sequence\n",
    "X_train, y_train = create_sequences(scaled_train, train_df['target'], sequence_length)\n",
    "X_test, y_test = create_sequences(scaled_test, test_df['target'], sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4273dd3a-f153-43ba-a996-2b419cbdb06c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 17:46:42.909074: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 974.4443 - val_loss: 81.6247 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 240.8208 - val_loss: 15.0579 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "6835/6835 [==============================] - 26s 4ms/step - loss: 217.0476 - val_loss: 35.7736 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "6835/6835 [==============================] - 31s 5ms/step - loss: 229.4323 - val_loss: 24.8036 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 205.3668 - val_loss: 15.4186 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "6835/6835 [==============================] - 26s 4ms/step - loss: 181.4888 - val_loss: 89.0368 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 169.0916 - val_loss: 10.7442 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "6835/6835 [==============================] - 26s 4ms/step - loss: 185.8880 - val_loss: 24.2162 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "6835/6835 [==============================] - 26s 4ms/step - loss: 178.9353 - val_loss: 11.1043 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 158.0912 - val_loss: 17.3139 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "6835/6835 [==============================] - 26s 4ms/step - loss: 154.6535 - val_loss: 21.0857 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 154.2337 - val_loss: 29.9797 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "6835/6835 [==============================] - 26s 4ms/step - loss: 97.6658 - val_loss: 9.7749 - lr: 2.0000e-04\n",
      "Epoch 14/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 96.0600 - val_loss: 9.3854 - lr: 2.0000e-04\n",
      "Epoch 15/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 95.7499 - val_loss: 9.3592 - lr: 2.0000e-04\n",
      "Epoch 16/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 97.6139 - val_loss: 9.4712 - lr: 2.0000e-04\n",
      "Epoch 17/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 95.9691 - val_loss: 9.7139 - lr: 2.0000e-04\n",
      "Epoch 18/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 95.4161 - val_loss: 9.1656 - lr: 2.0000e-04\n",
      "Epoch 19/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 94.8930 - val_loss: 9.4248 - lr: 2.0000e-04\n",
      "Epoch 20/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 96.1155 - val_loss: 20.7658 - lr: 2.0000e-04\n",
      "Epoch 21/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 95.7702 - val_loss: 11.8629 - lr: 2.0000e-04\n",
      "Epoch 22/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 93.0756 - val_loss: 9.1911 - lr: 2.0000e-04\n",
      "Epoch 23/150\n",
      "6835/6835 [==============================] - 26s 4ms/step - loss: 95.6854 - val_loss: 9.8411 - lr: 2.0000e-04\n",
      "Epoch 24/150\n",
      "6835/6835 [==============================] - 26s 4ms/step - loss: 89.0942 - val_loss: 8.9809 - lr: 1.0000e-04\n",
      "Epoch 25/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 89.6770 - val_loss: 9.6393 - lr: 1.0000e-04\n",
      "Epoch 26/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 88.8014 - val_loss: 8.9671 - lr: 1.0000e-04\n",
      "Epoch 27/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 89.3053 - val_loss: 8.8823 - lr: 1.0000e-04\n",
      "Epoch 28/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 89.2975 - val_loss: 8.8733 - lr: 1.0000e-04\n",
      "Epoch 29/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 89.1264 - val_loss: 9.3561 - lr: 1.0000e-04\n",
      "Epoch 30/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 88.7573 - val_loss: 9.2866 - lr: 1.0000e-04\n",
      "Epoch 31/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 90.0815 - val_loss: 9.2302 - lr: 1.0000e-04\n",
      "Epoch 32/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 88.5275 - val_loss: 8.8294 - lr: 1.0000e-04\n",
      "Epoch 33/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 89.2094 - val_loss: 9.3811 - lr: 1.0000e-04\n",
      "Epoch 34/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 88.5024 - val_loss: 8.9328 - lr: 1.0000e-04\n",
      "Epoch 35/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 88.0491 - val_loss: 8.9301 - lr: 1.0000e-04\n",
      "Epoch 36/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 87.9076 - val_loss: 8.8085 - lr: 1.0000e-04\n",
      "Epoch 37/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 87.8990 - val_loss: 9.5249 - lr: 1.0000e-04\n",
      "Epoch 38/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 88.4110 - val_loss: 8.8494 - lr: 1.0000e-04\n",
      "Epoch 39/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 88.5069 - val_loss: 9.0539 - lr: 1.0000e-04\n",
      "Epoch 40/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 87.7562 - val_loss: 8.7116 - lr: 1.0000e-04\n",
      "Epoch 41/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 88.6994 - val_loss: 9.4112 - lr: 1.0000e-04\n",
      "Epoch 42/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 87.3458 - val_loss: 8.8918 - lr: 1.0000e-04\n",
      "Epoch 43/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 88.6814 - val_loss: 8.9261 - lr: 1.0000e-04\n",
      "Epoch 44/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 87.4369 - val_loss: 8.8904 - lr: 1.0000e-04\n",
      "Epoch 45/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 87.7678 - val_loss: 8.6414 - lr: 1.0000e-04\n",
      "Epoch 46/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 87.9250 - val_loss: 8.6525 - lr: 1.0000e-04\n",
      "Epoch 47/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 87.4031 - val_loss: 9.2939 - lr: 1.0000e-04\n",
      "Epoch 48/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 87.5902 - val_loss: 8.6219 - lr: 1.0000e-04\n",
      "Epoch 49/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 87.2583 - val_loss: 8.5587 - lr: 1.0000e-04\n",
      "Epoch 50/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 88.3009 - val_loss: 9.3629 - lr: 1.0000e-04\n",
      "Epoch 51/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 86.8610 - val_loss: 8.6091 - lr: 1.0000e-04\n",
      "Epoch 52/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 87.4223 - val_loss: 8.5574 - lr: 1.0000e-04\n",
      "Epoch 53/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 87.5618 - val_loss: 8.5964 - lr: 1.0000e-04\n",
      "Epoch 54/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 86.4324 - val_loss: 8.7148 - lr: 1.0000e-04\n",
      "Epoch 55/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 86.7401 - val_loss: 8.4953 - lr: 1.0000e-04\n",
      "Epoch 56/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 86.6926 - val_loss: 8.9314 - lr: 1.0000e-04\n",
      "Epoch 57/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 86.9336 - val_loss: 8.4715 - lr: 1.0000e-04\n",
      "Epoch 58/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 87.4148 - val_loss: 8.5480 - lr: 1.0000e-04\n",
      "Epoch 59/150\n",
      "6835/6835 [==============================] - 29s 4ms/step - loss: 86.0813 - val_loss: 8.7844 - lr: 1.0000e-04\n",
      "Epoch 60/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 87.1177 - val_loss: 8.6019 - lr: 1.0000e-04\n",
      "Epoch 61/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 86.0449 - val_loss: 8.4721 - lr: 1.0000e-04\n",
      "Epoch 62/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 87.5324 - val_loss: 9.8816 - lr: 1.0000e-04\n",
      "Epoch 63/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 86.4136 - val_loss: 8.4401 - lr: 1.0000e-04\n",
      "Epoch 64/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 86.7807 - val_loss: 8.5626 - lr: 1.0000e-04\n",
      "Epoch 65/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 86.4033 - val_loss: 8.7286 - lr: 1.0000e-04\n",
      "Epoch 66/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 87.0218 - val_loss: 8.5497 - lr: 1.0000e-04\n",
      "Epoch 67/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 86.3478 - val_loss: 8.4359 - lr: 1.0000e-04\n",
      "Epoch 68/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 86.2950 - val_loss: 8.4128 - lr: 1.0000e-04\n",
      "Epoch 69/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 85.9115 - val_loss: 8.3665 - lr: 1.0000e-04\n",
      "Epoch 70/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 85.9736 - val_loss: 8.7666 - lr: 1.0000e-04\n",
      "Epoch 71/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 86.0792 - val_loss: 8.4795 - lr: 1.0000e-04\n",
      "Epoch 72/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 86.2396 - val_loss: 8.4055 - lr: 1.0000e-04\n",
      "Epoch 73/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 86.5959 - val_loss: 10.3217 - lr: 1.0000e-04\n",
      "Epoch 74/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 86.6486 - val_loss: 8.4348 - lr: 1.0000e-04\n",
      "Epoch 75/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 85.8047 - val_loss: 8.3727 - lr: 1.0000e-04\n",
      "Epoch 76/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 86.3040 - val_loss: 8.8292 - lr: 1.0000e-04\n",
      "Epoch 77/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 86.2007 - val_loss: 8.4130 - lr: 1.0000e-04\n",
      "Epoch 78/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 85.4264 - val_loss: 12.2099 - lr: 1.0000e-04\n",
      "Epoch 79/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 86.8417 - val_loss: 8.3984 - lr: 1.0000e-04\n",
      "Epoch 80/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 87.1244 - val_loss: 8.4351 - lr: 1.0000e-04\n",
      "Epoch 81/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 86.1063 - val_loss: 9.9473 - lr: 1.0000e-04\n",
      "Epoch 82/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 87.0373 - val_loss: 8.3102 - lr: 1.0000e-04\n",
      "Epoch 83/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 85.9423 - val_loss: 8.4445 - lr: 1.0000e-04\n",
      "Epoch 84/150\n",
      "6835/6835 [==============================] - 29s 4ms/step - loss: 86.8121 - val_loss: 8.3741 - lr: 1.0000e-04\n",
      "Epoch 85/150\n",
      "6835/6835 [==============================] - 26s 4ms/step - loss: 86.1770 - val_loss: 8.2990 - lr: 1.0000e-04\n",
      "Epoch 86/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 86.5931 - val_loss: 8.4385 - lr: 1.0000e-04\n",
      "Epoch 87/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 85.8735 - val_loss: 9.9885 - lr: 1.0000e-04\n",
      "Epoch 88/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 85.3665 - val_loss: 13.8733 - lr: 1.0000e-04\n",
      "Epoch 89/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 86.4888 - val_loss: 8.2974 - lr: 1.0000e-04\n",
      "Epoch 90/150\n",
      "6835/6835 [==============================] - 31s 4ms/step - loss: 85.8273 - val_loss: 8.4922 - lr: 1.0000e-04\n",
      "Epoch 91/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 85.9241 - val_loss: 8.2531 - lr: 1.0000e-04\n",
      "Epoch 92/150\n",
      "6835/6835 [==============================] - 29s 4ms/step - loss: 86.2299 - val_loss: 8.4772 - lr: 1.0000e-04\n",
      "Epoch 93/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 85.9161 - val_loss: 8.3685 - lr: 1.0000e-04\n",
      "Epoch 94/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 85.9769 - val_loss: 8.5119 - lr: 1.0000e-04\n",
      "Epoch 95/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 85.6264 - val_loss: 8.3316 - lr: 1.0000e-04\n",
      "Epoch 96/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 86.0124 - val_loss: 8.2368 - lr: 1.0000e-04\n",
      "Epoch 97/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 85.7594 - val_loss: 8.3349 - lr: 1.0000e-04\n",
      "Epoch 98/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 85.3720 - val_loss: 8.3073 - lr: 1.0000e-04\n",
      "Epoch 99/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 86.6116 - val_loss: 8.6304 - lr: 1.0000e-04\n",
      "Epoch 100/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 85.6943 - val_loss: 8.2166 - lr: 1.0000e-04\n",
      "Epoch 101/150\n",
      "6835/6835 [==============================] - 29s 4ms/step - loss: 85.9936 - val_loss: 8.3990 - lr: 1.0000e-04\n",
      "Epoch 102/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 86.1114 - val_loss: 8.1861 - lr: 1.0000e-04\n",
      "Epoch 103/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 85.6024 - val_loss: 8.4604 - lr: 1.0000e-04\n",
      "Epoch 104/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 85.6375 - val_loss: 8.4382 - lr: 1.0000e-04\n",
      "Epoch 105/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 85.4323 - val_loss: 8.3493 - lr: 1.0000e-04\n",
      "Epoch 106/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 85.6301 - val_loss: 8.2112 - lr: 1.0000e-04\n",
      "Epoch 107/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 85.3030 - val_loss: 8.2169 - lr: 1.0000e-04\n",
      "Epoch 108/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 86.3853 - val_loss: 8.7698 - lr: 1.0000e-04\n",
      "Epoch 109/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 85.3196 - val_loss: 8.3339 - lr: 1.0000e-04\n",
      "Epoch 110/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 85.3188 - val_loss: 8.2836 - lr: 1.0000e-04\n",
      "Epoch 111/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 85.8080 - val_loss: 8.9981 - lr: 1.0000e-04\n",
      "Epoch 112/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 85.2299 - val_loss: 8.4581 - lr: 1.0000e-04\n",
      "Epoch 113/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 85.4489 - val_loss: 8.2943 - lr: 1.0000e-04\n",
      "Epoch 114/150\n",
      "6835/6835 [==============================] - 27s 4ms/step - loss: 85.9921 - val_loss: 8.3309 - lr: 1.0000e-04\n",
      "Epoch 115/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 85.6217 - val_loss: 8.2306 - lr: 1.0000e-04\n",
      "Epoch 116/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 85.6387 - val_loss: 8.3641 - lr: 1.0000e-04\n",
      "Epoch 117/150\n",
      "6835/6835 [==============================] - 28s 4ms/step - loss: 85.1145 - val_loss: 8.3682 - lr: 1.0000e-04\n",
      "14721/14721 [==============================] - 11s 731us/step\n",
      "MAE: 3.4883731712254757, RMSE: 17.026421678805086\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(16,activation='relu'),\n",
    "    Dense(8,activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "#compile model\n",
    "model.compile(optimizer=Adam(), loss='mse')\n",
    "\n",
    "#train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=150,\n",
    "                    batch_size=248,  # Start with a larger batch size\n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "#model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# evaluate on test set\n",
    "predictions = model.predict(X_test)\n",
    "predictions = predictions.squeeze()\n",
    "evaluate_model(predictions, y_test)#evaluate model accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4f06782-ad38-40ef-acf9-b54edd8b4390",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(471072,)\n",
      "(471072,)\n",
      "[185.39633 186.55916 189.28351 188.32915 189.25061 189.79044 190.79895\n",
      " 190.754   191.58112 191.51753]\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)\n",
    "print(predictions.shape)\n",
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef33a12e-0e3d-44bd-8b0a-f940d96702a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1883216, 7, 24)\n",
      "(471072, 7, 24)\n",
      "(1883216,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32095690-a1f0-4ce9-b7da-7b59dd507b68",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "print(list(X_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "465a5630-713a-44bb-87dc-16069c18c839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 7, 128)            3200      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7, 64)             8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7, 64)             4160      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7, 32)             2080      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 7, 32)             1056      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 7, 16)             528       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 7, 8)              136       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 56)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 57        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,473\n",
      "Trainable params: 19,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "199910ae-963d-4da5-b9a7-bcd9d48cea8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./exported_model/assets\n"
     ]
    }
   ],
   "source": [
    "#save the model\n",
    "model_save_path = './exported_model'\n",
    "tf.saved_model.save(model, model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "319792f5-f977-4ae2-b317-299d96cd99dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fitted_scaler.joblib']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save scaling fit\n",
    "from joblib import dump\n",
    "\n",
    "# Assume `scaler` is your fitted MinMaxScaler\n",
    "dump(scaler, 'fitted_scaler.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0781afbc-9613-4d4d-9d48-9b3a4de2e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train/test sequences\n",
    "sequence_length = 30 # holds the length of each sequence\n",
    "X_train, y_train = create_sequences(scaled_train, train_df['target'], sequence_length)\n",
    "X_test, y_test = create_sequences(scaled_test, test_df['target'], sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f54e93cc-9bd7-4d57-9cca-21ad747876be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "3311/3311 [==============================] - 139s 42ms/step - loss: 23415.8340 - val_loss: 68.1347 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "3311/3311 [==============================] - 126s 38ms/step - loss: 16087.1494 - val_loss: 96.7285 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "3311/3311 [==============================] - 116s 35ms/step - loss: 12438.6797 - val_loss: 123.6766 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "3311/3311 [==============================] - 115s 35ms/step - loss: 10134.6182 - val_loss: 62.3744 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "3311/3311 [==============================] - 115s 35ms/step - loss: 8504.9453 - val_loss: 66.2030 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "3311/3311 [==============================] - 116s 35ms/step - loss: 7254.1577 - val_loss: 61.7375 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "3311/3311 [==============================] - 116s 35ms/step - loss: 6270.2437 - val_loss: 84.2939 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "3311/3311 [==============================] - 114s 34ms/step - loss: 5454.9644 - val_loss: 84.2996 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "3311/3311 [==============================] - 120s 36ms/step - loss: 4759.3916 - val_loss: 61.7603 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "3311/3311 [==============================] - 112s 34ms/step - loss: 4180.6421 - val_loss: 79.9211 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "3311/3311 [==============================] - 112s 34ms/step - loss: 3691.0845 - val_loss: 90.2124 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "3311/3311 [==============================] - 123s 37ms/step - loss: 3260.8416 - val_loss: 68.2691 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "3311/3311 [==============================] - 119s 36ms/step - loss: 2876.8855 - val_loss: 61.0076 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "3311/3311 [==============================] - 116s 35ms/step - loss: 2533.1853 - val_loss: 97.6190 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "3311/3311 [==============================] - 115s 35ms/step - loss: 2235.7910 - val_loss: 102.1262 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "3311/3311 [==============================] - 109s 33ms/step - loss: 1951.0225 - val_loss: 74.3157 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "3311/3311 [==============================] - 109s 33ms/step - loss: 1723.0618 - val_loss: 68.8997 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "3311/3311 [==============================] - 114s 34ms/step - loss: 1524.9978 - val_loss: 70.6131 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "3311/3311 [==============================] - 112s 34ms/step - loss: 1351.8369 - val_loss: 61.7725 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "3311/3311 [==============================] - 111s 33ms/step - loss: 1211.8181 - val_loss: 70.1842 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "3311/3311 [==============================] - 112s 34ms/step - loss: 1099.4824 - val_loss: 72.0855 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "3311/3311 [==============================] - 122s 37ms/step - loss: 1004.8654 - val_loss: 63.5044 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "3311/3311 [==============================] - 121s 37ms/step - loss: 923.9985 - val_loss: 66.4065 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "3311/3311 [==============================] - 111s 34ms/step - loss: 870.1997 - val_loss: 69.4960 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "3311/3311 [==============================] - 117s 35ms/step - loss: 805.3682 - val_loss: 185.7610 - lr: 0.0010\n",
      "Epoch 26/150\n",
      "3311/3311 [==============================] - 113s 34ms/step - loss: 778.3834 - val_loss: 65.0162 - lr: 0.0010\n",
      "Epoch 27/150\n",
      "3311/3311 [==============================] - 116s 35ms/step - loss: 745.6127 - val_loss: 91.9895 - lr: 0.0010\n",
      "Epoch 28/150\n",
      "3311/3311 [==============================] - 120s 36ms/step - loss: 720.8580 - val_loss: 68.5960 - lr: 0.0010\n",
      "14721/14721 [==============================] - 31s 2ms/step\n",
      "MAE: 11.580812377191757, RMSE: 60.859004206605654\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(128, activation='sigmoid'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='sigmoid'),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(32,activation='sigmoid'),\n",
    "    Dense(16,activation='relu'),\n",
    "    Dense(8,activation='sigmoid'),\n",
    "    Flatten(),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "#compile model\n",
    "model.compile(optimizer=Adam(), loss='mse')\n",
    "\n",
    "#train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=150,\n",
    "                    batch_size=512,  # Start with a larger batch size\n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "#model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# evaluate on test set\n",
    "predictions = model.predict(X_test)\n",
    "predictions = predictions.squeeze()\n",
    "evaluate_model(predictions, y_test)#evaluate model accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "839d161a-6304-438a-9f2e-760b45d655cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train/test sequences\n",
    "sequence_length = 15 # holds the length of each sequence\n",
    "X_train, y_train = create_sequences(scaled_train, train_df['target'], sequence_length)\n",
    "X_test, y_test = create_sequences(scaled_test, test_df['target'], sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9ddd880-aaa6-476d-a6b0-d31865871e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16949/16949 [==============================] - 103s 6ms/step - loss: 19603.1055 - val_loss: 49.6242 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "16949/16949 [==============================] - 97s 6ms/step - loss: 12284.1729 - val_loss: 38.6107 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "16949/16949 [==============================] - 95s 6ms/step - loss: 9350.7949 - val_loss: 163.5926 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "16949/16949 [==============================] - 94s 6ms/step - loss: 7525.7354 - val_loss: 86.0394 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "16949/16949 [==============================] - 94s 6ms/step - loss: 6188.2803 - val_loss: 53.8467 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "16949/16949 [==============================] - 102s 6ms/step - loss: 5155.9741 - val_loss: 42.8458 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "16949/16949 [==============================] - 98s 6ms/step - loss: 4335.6284 - val_loss: 42.3247 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "16949/16949 [==============================] - 95s 6ms/step - loss: 3671.3872 - val_loss: 44.9526 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "16949/16949 [==============================] - 92s 5ms/step - loss: 3129.1174 - val_loss: 41.6502 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "16949/16949 [==============================] - 101s 6ms/step - loss: 2656.5469 - val_loss: 40.3182 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "16949/16949 [==============================] - 107s 6ms/step - loss: 2248.7917 - val_loss: 44.6516 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "16949/16949 [==============================] - 94s 6ms/step - loss: 1904.5416 - val_loss: 40.5290 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "16949/16949 [==============================] - 94s 6ms/step - loss: 1601.6562 - val_loss: 41.8403 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "16949/16949 [==============================] - 95s 6ms/step - loss: 1341.4071 - val_loss: 44.3189 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "16949/16949 [==============================] - 96s 6ms/step - loss: 1129.6185 - val_loss: 106.7029 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "16949/16949 [==============================] - 97s 6ms/step - loss: 963.7488 - val_loss: 41.1563 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "16949/16949 [==============================] - 96s 6ms/step - loss: 819.7539 - val_loss: 45.1369 - lr: 0.0010\n",
      "14721/14721 [==============================] - 22s 1ms/step\n",
      "MAE: 16.619785052834295, RMSE: 165.53723442155925\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(128, activation='sigmoid'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='sigmoid'),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(32,activation='sigmoid'),\n",
    "    Dense(16,activation='relu'),\n",
    "    Dense(8,activation='sigmoid'),\n",
    "    Flatten(),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "#compile model\n",
    "model.compile(optimizer=Adam(), loss='mse')\n",
    "\n",
    "#train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,  # Start with a larger batch size\n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "#model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# evaluate on test set\n",
    "predictions = model.predict(X_test)\n",
    "predictions = predictions.squeeze()\n",
    "evaluate_model(predictions, y_test)#evaluate model accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cbee8fd-5a7f-4418-9207-f159a5956008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train/test sequences\n",
    "sequence_length = 45 # holds the length of each sequence\n",
    "X_train, y_train = create_sequences(scaled_train, train_df['target'], sequence_length)\n",
    "X_test, y_test = create_sequences(scaled_test, test_df['target'], sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "208b32a2-4cca-464c-b5cf-bbde94f6102a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 12:31:46.022510: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16949/16949 [==============================] - 113s 7ms/step - loss: 807.5663 - val_loss: 23.5057 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "16949/16949 [==============================] - 100s 6ms/step - loss: 314.9342 - val_loss: 19.9003 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "16949/16949 [==============================] - 102s 6ms/step - loss: 287.3748 - val_loss: 21.6230 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "16949/16949 [==============================] - 96s 6ms/step - loss: 245.9908 - val_loss: 22.9199 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "16949/16949 [==============================] - 96s 6ms/step - loss: 228.9581 - val_loss: 16.3764 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "16949/16949 [==============================] - 101s 6ms/step - loss: 224.1772 - val_loss: 13.0825 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "16949/16949 [==============================] - 92s 5ms/step - loss: 192.0335 - val_loss: 12.8621 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "16949/16949 [==============================] - 90s 5ms/step - loss: 192.2803 - val_loss: 45.2967 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "16949/16949 [==============================] - 92s 5ms/step - loss: 188.5408 - val_loss: 16.1435 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "16949/16949 [==============================] - 92s 5ms/step - loss: 183.0297 - val_loss: 11.4693 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "16949/16949 [==============================] - 100s 6ms/step - loss: 167.3516 - val_loss: 47.5917 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "16949/16949 [==============================] - 95s 6ms/step - loss: 165.0885 - val_loss: 10.9362 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "16949/16949 [==============================] - 89s 5ms/step - loss: 167.7306 - val_loss: 131.2905 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "16949/16949 [==============================] - 89s 5ms/step - loss: 155.1939 - val_loss: 11.6227 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "16949/16949 [==============================] - 103s 6ms/step - loss: 152.9467 - val_loss: 26.1916 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "16949/16949 [==============================] - 98s 6ms/step - loss: 148.7570 - val_loss: 10.7382 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "16949/16949 [==============================] - 91s 5ms/step - loss: 148.2358 - val_loss: 17.7727 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "16949/16949 [==============================] - 90s 5ms/step - loss: 143.0702 - val_loss: 11.3067 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "16949/16949 [==============================] - 89s 5ms/step - loss: 139.5512 - val_loss: 13.1975 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "16949/16949 [==============================] - 87s 5ms/step - loss: 141.6024 - val_loss: 13.2698 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "16949/16949 [==============================] - 98s 6ms/step - loss: 134.7777 - val_loss: 10.6208 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "16949/16949 [==============================] - 101s 6ms/step - loss: 138.5657 - val_loss: 9.5504 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "16949/16949 [==============================] - 92s 5ms/step - loss: 130.3089 - val_loss: 10.0920 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "16949/16949 [==============================] - 93s 6ms/step - loss: 135.6619 - val_loss: 10.1026 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "16949/16949 [==============================] - 96s 6ms/step - loss: 134.2209 - val_loss: 9.8062 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "16949/16949 [==============================] - 95s 6ms/step - loss: 130.1808 - val_loss: 10.3328 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "16949/16949 [==============================] - 95s 6ms/step - loss: 131.6137 - val_loss: 10.6041 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "16949/16949 [==============================] - 89s 5ms/step - loss: 130.9941 - val_loss: 15.4042 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "16949/16949 [==============================] - 91s 5ms/step - loss: 127.5966 - val_loss: 22.5232 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "16949/16949 [==============================] - 101s 6ms/step - loss: 122.5570 - val_loss: 11.4198 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "16949/16949 [==============================] - 97s 6ms/step - loss: 121.7672 - val_loss: 15.2921 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "16949/16949 [==============================] - 106s 6ms/step - loss: 120.6715 - val_loss: 10.5356 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "16949/16949 [==============================] - 101s 6ms/step - loss: 121.5207 - val_loss: 69.1299 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "16949/16949 [==============================] - 89s 5ms/step - loss: 123.2111 - val_loss: 9.1646 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "16949/16949 [==============================] - 87s 5ms/step - loss: 119.4293 - val_loss: 13.8047 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "16949/16949 [==============================] - 87s 5ms/step - loss: 118.0001 - val_loss: 10.4395 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "16949/16949 [==============================] - 87s 5ms/step - loss: 117.8919 - val_loss: 10.1342 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "16949/16949 [==============================] - 86s 5ms/step - loss: 116.1445 - val_loss: 10.2790 - lr: 0.0010\n",
      "Epoch 39/100\n",
      "16949/16949 [==============================] - 85s 5ms/step - loss: 116.1510 - val_loss: 11.8215 - lr: 0.0010\n",
      "Epoch 40/100\n",
      "16949/16949 [==============================] - 87s 5ms/step - loss: 118.0849 - val_loss: 9.1360 - lr: 0.0010\n",
      "Epoch 41/100\n",
      "16949/16949 [==============================] - 89s 5ms/step - loss: 117.2512 - val_loss: 10.3386 - lr: 0.0010\n",
      "Epoch 42/100\n",
      "16949/16949 [==============================] - 87s 5ms/step - loss: 118.0384 - val_loss: 12.1222 - lr: 0.0010\n",
      "Epoch 43/100\n",
      "16949/16949 [==============================] - 87s 5ms/step - loss: 115.1373 - val_loss: 9.5919 - lr: 0.0010\n",
      "Epoch 44/100\n",
      "16949/16949 [==============================] - 87s 5ms/step - loss: 117.4011 - val_loss: 9.1421 - lr: 0.0010\n",
      "Epoch 45/100\n",
      "16949/16949 [==============================] - 93s 5ms/step - loss: 114.1401 - val_loss: 10.7797 - lr: 0.0010\n",
      "Epoch 46/100\n",
      "16949/16949 [==============================] - 88s 5ms/step - loss: 113.7328 - val_loss: 10.9656 - lr: 0.0010\n",
      "Epoch 47/100\n",
      "16949/16949 [==============================] - 100s 6ms/step - loss: 117.7028 - val_loss: 22.1722 - lr: 0.0010\n",
      "Epoch 48/100\n",
      "16949/16949 [==============================] - 90s 5ms/step - loss: 113.7348 - val_loss: 8.6474 - lr: 0.0010\n",
      "Epoch 49/100\n",
      "16949/16949 [==============================] - 86s 5ms/step - loss: 115.0668 - val_loss: 10.2365 - lr: 0.0010\n",
      "Epoch 50/100\n",
      "16949/16949 [==============================] - 87s 5ms/step - loss: 114.3822 - val_loss: 10.8771 - lr: 0.0010\n",
      "Epoch 51/100\n",
      "16949/16949 [==============================] - 90s 5ms/step - loss: 113.4754 - val_loss: 9.3423 - lr: 0.0010\n",
      "Epoch 52/100\n",
      "16949/16949 [==============================] - 84s 5ms/step - loss: 112.0865 - val_loss: 9.3673 - lr: 0.0010\n",
      "Epoch 53/100\n",
      "16949/16949 [==============================] - 83s 5ms/step - loss: 111.0337 - val_loss: 14.4732 - lr: 0.0010\n",
      "Epoch 54/100\n",
      "16949/16949 [==============================] - 83s 5ms/step - loss: 111.6435 - val_loss: 13.8906 - lr: 0.0010\n",
      "Epoch 55/100\n",
      "16949/16949 [==============================] - 83s 5ms/step - loss: 112.2422 - val_loss: 9.0006 - lr: 0.0010\n",
      "Epoch 56/100\n",
      "16949/16949 [==============================] - 89s 5ms/step - loss: 116.5476 - val_loss: 12.4332 - lr: 0.0010\n",
      "Epoch 57/100\n",
      "16949/16949 [==============================] - 86s 5ms/step - loss: 111.1731 - val_loss: 10.2478 - lr: 0.0010\n",
      "Epoch 58/100\n",
      "16949/16949 [==============================] - 86s 5ms/step - loss: 111.2425 - val_loss: 8.9227 - lr: 0.0010\n",
      "Epoch 59/100\n",
      "16949/16949 [==============================] - 84s 5ms/step - loss: 112.2272 - val_loss: 13.5086 - lr: 0.0010\n",
      "Epoch 60/100\n",
      "16949/16949 [==============================] - 85s 5ms/step - loss: 109.2862 - val_loss: 8.9945 - lr: 0.0010\n",
      "Epoch 61/100\n",
      "16949/16949 [==============================] - 83s 5ms/step - loss: 111.0804 - val_loss: 8.7502 - lr: 0.0010\n",
      "Epoch 62/100\n",
      "16949/16949 [==============================] - 83s 5ms/step - loss: 108.9627 - val_loss: 8.9870 - lr: 0.0010\n",
      "Epoch 63/100\n",
      "16949/16949 [==============================] - 86s 5ms/step - loss: 109.0212 - val_loss: 9.2650 - lr: 0.0010\n",
      "14720/14720 [==============================] - 19s 1ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     32\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m---> 33\u001b[0m \u001b[43mevaluate_model\u001b[49m(predictions, y_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate_model' is not defined"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    #Dense(128, activation='sigmoid'),\n",
    "    Dense(64, activation='relu'),\n",
    "    #Dense(64, activation='sigmoid'),\n",
    "    Dense(32,activation='relu'),\n",
    "    #Dense(32,activation='sigmoid'),\n",
    "    Dense(16,activation='relu'),\n",
    "    #Dense(8,activation='sigmoid'),\n",
    "    Flatten(),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "#compile model\n",
    "model.compile(optimizer=Adam(), loss='mse')\n",
    "\n",
    "#train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,  # Start with a larger batch size\n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "#model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# evaluate on test set\n",
    "predictions = model.predict(X_test)\n",
    "predictions = predictions.squeeze()\n",
    "evaluate_model(predictions, y_test)#evaluate model accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "036fccf2-0065-4733-b69c-e7c28f02c25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.6511066933893126, RMSE: 17.60244346683435\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(predictions, y_test)#evaluate model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d912810f-dd52-4a40-aace-6a43a412576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train/test sequences\n",
    "sequence_length = 10 # holds the length of each sequence\n",
    "X_train, y_train = create_sequences(scaled_train, train_df['target'], sequence_length)\n",
    "X_test, y_test = create_sequences(scaled_test, test_df['target'], sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "564284fa-dbe4-46c0-8341-583d94485bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16949/16949 [==============================] - 41s 2ms/step - loss: 30619.4512 - val_loss: 3467.3396 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "16949/16949 [==============================] - 38s 2ms/step - loss: 29353.3555 - val_loss: 2726.4075 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "16949/16949 [==============================] - 38s 2ms/step - loss: 28612.5645 - val_loss: 2440.5562 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "16949/16949 [==============================] - 37s 2ms/step - loss: 28302.1719 - val_loss: 2420.7710 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "16949/16949 [==============================] - 37s 2ms/step - loss: 28223.0293 - val_loss: 2454.5122 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "16949/16949 [==============================] - 37s 2ms/step - loss: 28210.1973 - val_loss: 2473.2710 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "16949/16949 [==============================] - 37s 2ms/step - loss: 28208.3965 - val_loss: 2481.6836 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "16949/16949 [==============================] - 40s 2ms/step - loss: 28208.1836 - val_loss: 2485.2544 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "16949/16949 [==============================] - 37s 2ms/step - loss: 28208.1152 - val_loss: 2485.5181 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "16949/16949 [==============================] - 36s 2ms/step - loss: 28208.0566 - val_loss: 2486.1448 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "16949/16949 [==============================] - 36s 2ms/step - loss: 28208.1680 - val_loss: 2486.2913 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "16949/16949 [==============================] - 36s 2ms/step - loss: 28208.0840 - val_loss: 2486.8357 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "16949/16949 [==============================] - 36s 2ms/step - loss: 28208.1172 - val_loss: 2487.3438 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "16949/16949 [==============================] - 36s 2ms/step - loss: 28208.1484 - val_loss: 2487.2266 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "16949/16949 [==============================] - 37s 2ms/step - loss: 28208.0039 - val_loss: 2487.7578 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "16949/16949 [==============================] - 36s 2ms/step - loss: 28208.1094 - val_loss: 2486.5110 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "16949/16949 [==============================] - 37s 2ms/step - loss: 28208.1211 - val_loss: 2486.6316 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "16949/16949 [==============================] - 36s 2ms/step - loss: 28208.0762 - val_loss: 2487.1445 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "16949/16949 [==============================] - 35s 2ms/step - loss: 28208.0352 - val_loss: 2486.8359 - lr: 0.0010\n",
      "14721/14721 [==============================] - 11s 723us/step\n",
      "MAE: 115.95463194885325, RMSE: 308.7345684252839\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(16,activation='relu'),\n",
    "    Dense(8,activation='relu'),\n",
    "    Dense(4,activation='relu'),\n",
    "    Dense(2,activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "#compile model\n",
    "model.compile(optimizer=Adam(), loss='mse')\n",
    "\n",
    "#train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,  # Start with a larger batch size\n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "#model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# evaluate on test set\n",
    "predictions = model.predict(X_test)\n",
    "predictions = predictions.squeeze()\n",
    "evaluate_model(predictions, y_test)#evaluate model accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bc7c874-cb0e-4704-893c-f574af98b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train/test sequences\n",
    "sequence_length = 10 # holds the length of each sequence\n",
    "X_train, y_train = create_sequences(scaled_train, train_df['target'], sequence_length)\n",
    "X_test, y_test = create_sequences(scaled_test, test_df['target'], sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48ddd3bc-16d6-4042-a0e6-c2484fd897d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16949/16949 [==============================] - 75s 4ms/step - loss: 805.1049 - val_loss: 43.8564 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "16949/16949 [==============================] - 78s 5ms/step - loss: 310.5793 - val_loss: 15.6786 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "16949/16949 [==============================] - 73s 4ms/step - loss: 250.3736 - val_loss: 21.8742 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "16949/16949 [==============================] - 73s 4ms/step - loss: 248.6731 - val_loss: 11.9359 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "16949/16949 [==============================] - 76s 4ms/step - loss: 208.5689 - val_loss: 10.9322 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "16949/16949 [==============================] - 78s 5ms/step - loss: 195.8464 - val_loss: 14.3706 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "16949/16949 [==============================] - 79s 5ms/step - loss: 170.9386 - val_loss: 11.7647 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "16949/16949 [==============================] - 76s 4ms/step - loss: 175.7600 - val_loss: 13.1456 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "16949/16949 [==============================] - 72s 4ms/step - loss: 163.5742 - val_loss: 31.0721 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "16949/16949 [==============================] - 72s 4ms/step - loss: 170.3174 - val_loss: 9.6371 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "16949/16949 [==============================] - 72s 4ms/step - loss: 148.5014 - val_loss: 9.4477 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "16949/16949 [==============================] - 72s 4ms/step - loss: 148.8758 - val_loss: 10.2891 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "16949/16949 [==============================] - 73s 4ms/step - loss: 145.7040 - val_loss: 17.6200 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "16949/16949 [==============================] - 72s 4ms/step - loss: 144.9811 - val_loss: 17.7157 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "16949/16949 [==============================] - 72s 4ms/step - loss: 142.6045 - val_loss: 10.5957 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "16949/16949 [==============================] - 72s 4ms/step - loss: 140.4634 - val_loss: 15.4018 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "16949/16949 [==============================] - 73s 4ms/step - loss: 134.1144 - val_loss: 24.2700 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "16949/16949 [==============================] - 72s 4ms/step - loss: 136.7655 - val_loss: 10.1380 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "16949/16949 [==============================] - 72s 4ms/step - loss: 138.1474 - val_loss: 10.1289 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "16949/16949 [==============================] - 72s 4ms/step - loss: 130.3620 - val_loss: 10.4922 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "16949/16949 [==============================] - 72s 4ms/step - loss: 131.4560 - val_loss: 10.4425 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "16949/16949 [==============================] - 72s 4ms/step - loss: 125.8884 - val_loss: 30.2441 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "16949/16949 [==============================] - 73s 4ms/step - loss: 132.9431 - val_loss: 31.5371 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "16949/16949 [==============================] - 77s 5ms/step - loss: 128.8701 - val_loss: 9.1083 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "16949/16949 [==============================] - 80s 5ms/step - loss: 127.6869 - val_loss: 13.9756 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "16949/16949 [==============================] - 73s 4ms/step - loss: 122.7221 - val_loss: 47.7604 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "16949/16949 [==============================] - 74s 4ms/step - loss: 127.7193 - val_loss: 9.6880 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "16949/16949 [==============================] - 73s 4ms/step - loss: 126.8323 - val_loss: 9.0972 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "16949/16949 [==============================] - 84s 5ms/step - loss: 122.6890 - val_loss: 9.6769 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "16949/16949 [==============================] - 75s 4ms/step - loss: 124.9668 - val_loss: 9.2041 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "16949/16949 [==============================] - 78s 5ms/step - loss: 120.9336 - val_loss: 8.8711 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "16949/16949 [==============================] - 75s 4ms/step - loss: 122.8373 - val_loss: 8.9847 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "16949/16949 [==============================] - 73s 4ms/step - loss: 116.6601 - val_loss: 9.1445 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "16949/16949 [==============================] - 76s 4ms/step - loss: 123.4459 - val_loss: 8.8080 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "16949/16949 [==============================] - 70s 4ms/step - loss: 124.8817 - val_loss: 14.0712 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "16949/16949 [==============================] - 71s 4ms/step - loss: 121.4066 - val_loss: 8.6456 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "16949/16949 [==============================] - 79s 5ms/step - loss: 121.2732 - val_loss: 10.5696 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "16949/16949 [==============================] - 77s 5ms/step - loss: 113.0303 - val_loss: 9.6022 - lr: 0.0010\n",
      "Epoch 39/100\n",
      "16949/16949 [==============================] - 76s 4ms/step - loss: 116.4939 - val_loss: 16.6241 - lr: 0.0010\n",
      "Epoch 40/100\n",
      "16949/16949 [==============================] - 76s 4ms/step - loss: 115.2184 - val_loss: 8.5988 - lr: 0.0010\n",
      "Epoch 41/100\n",
      "16949/16949 [==============================] - 74s 4ms/step - loss: 115.6680 - val_loss: 10.2174 - lr: 0.0010\n",
      "Epoch 42/100\n",
      "16949/16949 [==============================] - 77s 5ms/step - loss: 116.8227 - val_loss: 9.2358 - lr: 0.0010\n",
      "Epoch 43/100\n",
      "16949/16949 [==============================] - 73s 4ms/step - loss: 115.1800 - val_loss: 10.0442 - lr: 0.0010\n",
      "Epoch 44/100\n",
      "16949/16949 [==============================] - 72s 4ms/step - loss: 111.5402 - val_loss: 8.7876 - lr: 0.0010\n",
      "Epoch 45/100\n",
      "16949/16949 [==============================] - 71s 4ms/step - loss: 112.9459 - val_loss: 9.6828 - lr: 0.0010\n",
      "Epoch 46/100\n",
      "16949/16949 [==============================] - 71s 4ms/step - loss: 112.2631 - val_loss: 16.3449 - lr: 0.0010\n",
      "Epoch 47/100\n",
      "16949/16949 [==============================] - 71s 4ms/step - loss: 114.2966 - val_loss: 9.2908 - lr: 0.0010\n",
      "Epoch 48/100\n",
      "16949/16949 [==============================] - 71s 4ms/step - loss: 114.1521 - val_loss: 8.5603 - lr: 0.0010\n",
      "Epoch 49/100\n",
      "16949/16949 [==============================] - 74s 4ms/step - loss: 108.8776 - val_loss: 8.5375 - lr: 0.0010\n",
      "Epoch 50/100\n",
      "16949/16949 [==============================] - 74s 4ms/step - loss: 108.4676 - val_loss: 8.9639 - lr: 0.0010\n",
      "Epoch 51/100\n",
      "16949/16949 [==============================] - 77s 5ms/step - loss: 107.2609 - val_loss: 8.5808 - lr: 0.0010\n",
      "Epoch 52/100\n",
      "16949/16949 [==============================] - 77s 5ms/step - loss: 110.5313 - val_loss: 9.2215 - lr: 0.0010\n",
      "Epoch 53/100\n",
      "16949/16949 [==============================] - 76s 4ms/step - loss: 107.6824 - val_loss: 8.5386 - lr: 0.0010\n",
      "Epoch 54/100\n",
      "16949/16949 [==============================] - 76s 4ms/step - loss: 108.6967 - val_loss: 8.9453 - lr: 0.0010\n",
      "Epoch 55/100\n",
      "16949/16949 [==============================] - 74s 4ms/step - loss: 112.0426 - val_loss: 9.6083 - lr: 0.0010\n",
      "Epoch 56/100\n",
      "16949/16949 [==============================] - 76s 4ms/step - loss: 108.4192 - val_loss: 9.3739 - lr: 0.0010\n",
      "Epoch 57/100\n",
      "16949/16949 [==============================] - 80s 5ms/step - loss: 106.3249 - val_loss: 8.4126 - lr: 0.0010\n",
      "Epoch 58/100\n",
      "16949/16949 [==============================] - 78s 5ms/step - loss: 108.0352 - val_loss: 11.0135 - lr: 0.0010\n",
      "Epoch 59/100\n",
      "16949/16949 [==============================] - 78s 5ms/step - loss: 105.3889 - val_loss: 8.4263 - lr: 0.0010\n",
      "Epoch 60/100\n",
      "16949/16949 [==============================] - 78s 5ms/step - loss: 108.6350 - val_loss: 9.0510 - lr: 0.0010\n",
      "Epoch 61/100\n",
      "16949/16949 [==============================] - 75s 4ms/step - loss: 109.7224 - val_loss: 8.5933 - lr: 0.0010\n",
      "Epoch 62/100\n",
      "16949/16949 [==============================] - 74s 4ms/step - loss: 106.8953 - val_loss: 8.5382 - lr: 0.0010\n",
      "Epoch 63/100\n",
      "16949/16949 [==============================] - 74s 4ms/step - loss: 111.4221 - val_loss: 8.5516 - lr: 0.0010\n",
      "Epoch 64/100\n",
      "16949/16949 [==============================] - 74s 4ms/step - loss: 104.6332 - val_loss: 9.5893 - lr: 0.0010\n",
      "Epoch 65/100\n",
      "16949/16949 [==============================] - 76s 4ms/step - loss: 106.0429 - val_loss: 9.2201 - lr: 0.0010\n",
      "Epoch 66/100\n",
      "16949/16949 [==============================] - 74s 4ms/step - loss: 106.8877 - val_loss: 8.5776 - lr: 0.0010\n",
      "Epoch 67/100\n",
      "16949/16949 [==============================] - 85s 5ms/step - loss: 107.3534 - val_loss: 24.3279 - lr: 0.0010\n",
      "Epoch 68/100\n",
      "16949/16949 [==============================] - 76s 5ms/step - loss: 105.7552 - val_loss: 13.0167 - lr: 0.0010\n",
      "Epoch 69/100\n",
      "16949/16949 [==============================] - 75s 4ms/step - loss: 105.4446 - val_loss: 9.0991 - lr: 0.0010\n",
      "Epoch 70/100\n",
      "16949/16949 [==============================] - 74s 4ms/step - loss: 102.0033 - val_loss: 8.8711 - lr: 0.0010\n",
      "Epoch 71/100\n",
      "16949/16949 [==============================] - 75s 4ms/step - loss: 109.3259 - val_loss: 8.5209 - lr: 0.0010\n",
      "Epoch 72/100\n",
      "16949/16949 [==============================] - 75s 4ms/step - loss: 106.6136 - val_loss: 8.5357 - lr: 0.0010\n",
      "14721/14721 [==============================] - 16s 1ms/step\n",
      "MAE: 3.6653616723915663, RMSE: 17.177602896856314\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(16,activation='relu'),\n",
    "    Dense(8,activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "#compile model\n",
    "model.compile(optimizer=Adam(), loss='mse')\n",
    "\n",
    "#train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,  # Start with a larger batch size\n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "#model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# evaluate on test set\n",
    "predictions = model.predict(X_test)\n",
    "predictions = predictions.squeeze()\n",
    "evaluate_model(predictions, y_test)#evaluate model accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ff524ad-3fd4-40cf-8e99-70c48771081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train/test sequences\n",
    "sequence_length = 45 # holds the length of each sequence\n",
    "X_train, y_train = create_sequences(scaled_train, train_df['target'], sequence_length)\n",
    "X_test, y_test = create_sequences(scaled_test, test_df['target'], sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "148c1a6b-df13-4936-9ad9-aec2b82289ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 23:11:14.731168: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16134/16134 [==============================] - 213s 13ms/step - loss: 15139.2891 - val_loss: 130.3468 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "16134/16134 [==============================] - 200s 12ms/step - loss: 7453.7202 - val_loss: 107.6562 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "16134/16134 [==============================] - 197s 12ms/step - loss: 4807.6934 - val_loss: 130.1246 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "16134/16134 [==============================] - 194s 12ms/step - loss: 3330.7817 - val_loss: 106.9492 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "16134/16134 [==============================] - 196s 12ms/step - loss: 2315.7869 - val_loss: 272.7311 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "16134/16134 [==============================] - 196s 12ms/step - loss: 1681.5916 - val_loss: 111.2111 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "16134/16134 [==============================] - 198s 12ms/step - loss: 1347.6637 - val_loss: 109.1966 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "16134/16134 [==============================] - 195s 12ms/step - loss: 1187.2383 - val_loss: 138.1558 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "16134/16134 [==============================] - 195s 12ms/step - loss: 1104.2340 - val_loss: 89.1155 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "16134/16134 [==============================] - 197s 12ms/step - loss: 1046.9899 - val_loss: 86.0115 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "16134/16134 [==============================] - 195s 12ms/step - loss: 1031.2185 - val_loss: 113.0112 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "16134/16134 [==============================] - 195s 12ms/step - loss: 975.2667 - val_loss: 155.0701 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "16134/16134 [==============================] - 340s 21ms/step - loss: 937.6461 - val_loss: 74.9261 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "16134/16134 [==============================] - 205s 13ms/step - loss: 922.8251 - val_loss: 71.6285 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "16134/16134 [==============================] - 188s 12ms/step - loss: 886.1188 - val_loss: 204.0981 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "16134/16134 [==============================] - 186s 12ms/step - loss: 881.9747 - val_loss: 83.9657 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "16134/16134 [==============================] - 2808s 174ms/step - loss: 867.4706 - val_loss: 71.6235 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "16134/16134 [==============================] - 184s 11ms/step - loss: 832.0205 - val_loss: 75.5523 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "16134/16134 [==============================] - 186s 12ms/step - loss: 817.7378 - val_loss: 91.5127 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "16134/16134 [==============================] - 1851s 115ms/step - loss: 797.5191 - val_loss: 66.5284 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "16134/16134 [==============================] - 217s 13ms/step - loss: 774.7524 - val_loss: 66.4959 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "16134/16134 [==============================] - 199s 12ms/step - loss: 765.7141 - val_loss: 61.4832 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "16134/16134 [==============================] - 191s 12ms/step - loss: 769.9908 - val_loss: 59.7011 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "16134/16134 [==============================] - 187s 12ms/step - loss: 741.3683 - val_loss: 65.0025 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "16134/16134 [==============================] - 204s 13ms/step - loss: 755.1701 - val_loss: 74.9005 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "16134/16134 [==============================] - 1432s 89ms/step - loss: 704.0692 - val_loss: 54.5709 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "16134/16134 [==============================] - 1489s 92ms/step - loss: 696.6127 - val_loss: 59.7500 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "16134/16134 [==============================] - 1184s 73ms/step - loss: 715.6624 - val_loss: 58.9932 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "16134/16134 [==============================] - 658s 41ms/step - loss: 697.9933 - val_loss: 57.2603 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "16134/16134 [==============================] - 1711s 106ms/step - loss: 699.4085 - val_loss: 48.3194 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "16134/16134 [==============================] - 350s 22ms/step - loss: 705.2728 - val_loss: 47.9780 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "16134/16134 [==============================] - 1126s 70ms/step - loss: 652.2756 - val_loss: 68.4731 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "16134/16134 [==============================] - 1305s 81ms/step - loss: 638.1276 - val_loss: 50.3161 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "16134/16134 [==============================] - 204s 13ms/step - loss: 618.0488 - val_loss: 47.6656 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "16134/16134 [==============================] - 2048s 127ms/step - loss: 611.4122 - val_loss: 60.9496 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "16134/16134 [==============================] - 192s 12ms/step - loss: 600.5803 - val_loss: 47.4392 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "16134/16134 [==============================] - 197s 12ms/step - loss: 605.8819 - val_loss: 46.2861 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "16134/16134 [==============================] - 199s 12ms/step - loss: 584.6842 - val_loss: 44.4890 - lr: 0.0010\n",
      "Epoch 39/100\n",
      "16134/16134 [==============================] - 211s 13ms/step - loss: 573.6395 - val_loss: 43.9007 - lr: 0.0010\n",
      "Epoch 40/100\n",
      "16134/16134 [==============================] - 223s 14ms/step - loss: 633.4252 - val_loss: 52.1292 - lr: 0.0010\n",
      "Epoch 41/100\n",
      "16134/16134 [==============================] - 231s 14ms/step - loss: 600.0741 - val_loss: 41.0682 - lr: 0.0010\n",
      "Epoch 42/100\n",
      "16134/16134 [==============================] - 239s 15ms/step - loss: 588.8313 - val_loss: 42.4886 - lr: 0.0010\n",
      "Epoch 43/100\n",
      "16134/16134 [==============================] - 245s 15ms/step - loss: 694.4205 - val_loss: 39.1789 - lr: 0.0010\n",
      "Epoch 44/100\n",
      "16134/16134 [==============================] - 251s 16ms/step - loss: 585.1773 - val_loss: 41.5206 - lr: 0.0010\n",
      "Epoch 45/100\n",
      "16134/16134 [==============================] - 256s 16ms/step - loss: 560.2525 - val_loss: 39.1298 - lr: 0.0010\n",
      "Epoch 46/100\n",
      "16134/16134 [==============================] - 264s 16ms/step - loss: 546.3221 - val_loss: 39.0367 - lr: 0.0010\n",
      "Epoch 47/100\n",
      "16134/16134 [==============================] - 269s 17ms/step - loss: 552.3187 - val_loss: 40.2754 - lr: 0.0010\n",
      "Epoch 48/100\n",
      "16134/16134 [==============================] - 283s 18ms/step - loss: 546.0720 - val_loss: 39.4795 - lr: 0.0010\n",
      "Epoch 49/100\n",
      "16134/16134 [==============================] - 274s 17ms/step - loss: 527.1332 - val_loss: 37.2590 - lr: 0.0010\n",
      "Epoch 50/100\n",
      "16134/16134 [==============================] - 423s 26ms/step - loss: 529.1124 - val_loss: 45.7413 - lr: 0.0010\n",
      "Epoch 51/100\n",
      "16134/16134 [==============================] - 1097s 68ms/step - loss: 545.1090 - val_loss: 74.2693 - lr: 0.0010\n",
      "Epoch 52/100\n",
      "16134/16134 [==============================] - 1155s 72ms/step - loss: 526.6689 - val_loss: 45.5130 - lr: 0.0010\n",
      "Epoch 53/100\n",
      "16134/16134 [==============================] - 202s 13ms/step - loss: 517.7883 - val_loss: 44.3489 - lr: 0.0010\n",
      "Epoch 54/100\n",
      "16134/16134 [==============================] - 206s 13ms/step - loss: 514.4358 - val_loss: 61.8510 - lr: 0.0010\n",
      "Epoch 55/100\n",
      "16134/16134 [==============================] - 234s 14ms/step - loss: 512.8720 - val_loss: 39.0445 - lr: 0.0010\n",
      "Epoch 56/100\n",
      "16134/16134 [==============================] - 235s 15ms/step - loss: 516.2679 - val_loss: 54.9378 - lr: 0.0010\n",
      "Epoch 57/100\n",
      "16134/16134 [==============================] - 247s 15ms/step - loss: 502.9339 - val_loss: 42.2850 - lr: 0.0010\n",
      "Epoch 58/100\n",
      "16134/16134 [==============================] - 271s 17ms/step - loss: 472.0103 - val_loss: 41.8587 - lr: 0.0010\n",
      "Epoch 59/100\n",
      "16134/16134 [==============================] - 285s 18ms/step - loss: 489.5042 - val_loss: 34.9147 - lr: 0.0010\n",
      "Epoch 60/100\n",
      "16134/16134 [==============================] - 286s 18ms/step - loss: 489.2794 - val_loss: 50.5709 - lr: 0.0010\n",
      "Epoch 61/100\n",
      "16134/16134 [==============================] - 273s 17ms/step - loss: 472.8929 - val_loss: 65.9772 - lr: 0.0010\n",
      "Epoch 62/100\n",
      "16134/16134 [==============================] - 1182s 73ms/step - loss: 468.7371 - val_loss: 39.5496 - lr: 0.0010\n",
      "Epoch 63/100\n",
      "16134/16134 [==============================] - 3124s 194ms/step - loss: 474.4804 - val_loss: 35.5363 - lr: 0.0010\n",
      "Epoch 64/100\n",
      "16134/16134 [==============================] - 1105s 69ms/step - loss: 465.3310 - val_loss: 34.4635 - lr: 0.0010\n",
      "Epoch 65/100\n",
      "16134/16134 [==============================] - 383s 24ms/step - loss: 494.8445 - val_loss: 34.2056 - lr: 0.0010\n",
      "Epoch 66/100\n",
      "16134/16134 [==============================] - 1014s 63ms/step - loss: 448.7474 - val_loss: 46.2457 - lr: 0.0010\n",
      "Epoch 67/100\n",
      "16134/16134 [==============================] - 1662s 103ms/step - loss: 453.5412 - val_loss: 47.1800 - lr: 0.0010\n",
      "Epoch 68/100\n",
      "16134/16134 [==============================] - 194s 12ms/step - loss: 470.4132 - val_loss: 42.7628 - lr: 0.0010\n",
      "Epoch 69/100\n",
      "16134/16134 [==============================] - 1657s 103ms/step - loss: 439.0250 - val_loss: 50.9650 - lr: 0.0010\n",
      "Epoch 70/100\n",
      "16134/16134 [==============================] - 1203s 75ms/step - loss: 457.9445 - val_loss: 34.1543 - lr: 0.0010\n",
      "Epoch 71/100\n",
      "16134/16134 [==============================] - 1136s 70ms/step - loss: 462.7839 - val_loss: 33.8789 - lr: 0.0010\n",
      "Epoch 72/100\n",
      "16134/16134 [==============================] - 1600s 99ms/step - loss: 477.8549 - val_loss: 32.4249 - lr: 0.0010\n",
      "Epoch 73/100\n",
      "16134/16134 [==============================] - 647s 40ms/step - loss: 595.8550 - val_loss: 59.5060 - lr: 0.0010\n",
      "Epoch 74/100\n",
      "16134/16134 [==============================] - 218s 13ms/step - loss: 536.3000 - val_loss: 37.7799 - lr: 0.0010\n",
      "Epoch 75/100\n",
      "16134/16134 [==============================] - 209s 13ms/step - loss: 448.8812 - val_loss: 42.1318 - lr: 0.0010\n",
      "Epoch 76/100\n",
      "16134/16134 [==============================] - 220s 14ms/step - loss: 440.4648 - val_loss: 42.0063 - lr: 0.0010\n",
      "Epoch 77/100\n",
      "16134/16134 [==============================] - 221s 14ms/step - loss: 434.1484 - val_loss: 39.4515 - lr: 0.0010\n",
      "Epoch 78/100\n",
      "16134/16134 [==============================] - 215s 13ms/step - loss: 427.1786 - val_loss: 59.4178 - lr: 0.0010\n",
      "Epoch 79/100\n",
      "16134/16134 [==============================] - 222s 14ms/step - loss: 419.0788 - val_loss: 43.7823 - lr: 0.0010\n",
      "Epoch 80/100\n",
      "16134/16134 [==============================] - 490s 30ms/step - loss: 432.1462 - val_loss: 57.6187 - lr: 0.0010\n",
      "Epoch 81/100\n",
      "16134/16134 [==============================] - 419s 26ms/step - loss: 419.8251 - val_loss: 46.3024 - lr: 0.0010\n",
      "Epoch 82/100\n",
      "16134/16134 [==============================] - 223s 14ms/step - loss: 411.5099 - val_loss: 71.3973 - lr: 0.0010\n",
      "Epoch 83/100\n",
      "16134/16134 [==============================] - 217s 13ms/step - loss: 418.8789 - val_loss: 37.3291 - lr: 0.0010\n",
      "Epoch 84/100\n",
      "16134/16134 [==============================] - 215s 13ms/step - loss: 399.3386 - val_loss: 87.0291 - lr: 0.0010\n",
      "Epoch 85/100\n",
      "16134/16134 [==============================] - 221s 14ms/step - loss: 399.8686 - val_loss: 33.6560 - lr: 0.0010\n",
      "Epoch 86/100\n",
      "16134/16134 [==============================] - 216s 13ms/step - loss: 409.5111 - val_loss: 33.4609 - lr: 0.0010\n",
      "Epoch 87/100\n",
      "16134/16134 [==============================] - 225s 14ms/step - loss: 400.8338 - val_loss: 43.6509 - lr: 0.0010\n",
      "14012/14012 [==============================] - 40s 3ms/step\n",
      "MAE: 8.63937758740406, RMSE: 51.20509828999311\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(128, activation='sigmoid'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='sigmoid'),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(32,activation='sigmoid'),\n",
    "    Dense(16,activation='relu'),\n",
    "    Dense(8,activation='sigmoid'),\n",
    "    Flatten(),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "#compile model\n",
    "model.compile(optimizer=Adam(), loss='mse')\n",
    "\n",
    "#train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,  # Start with a larger batch size\n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "#model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# evaluate on test set\n",
    "predictions = model.predict(X_test)\n",
    "predictions = predictions.squeeze()\n",
    "evaluate_model(predictions, y_test)#evaluate model accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afb818a2-e91a-4c7a-a841-27413ddbadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train/test sequences\n",
    "sequence_length = 21 # holds the length of each sequence\n",
    "X_train, y_train = create_sequences(scaled_train, train_df['target'], sequence_length)\n",
    "X_test, y_test = create_sequences(scaled_test, test_df['target'], sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a43bb39c-4a76-42b0-baf5-9da0de8a4203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "6506/6506 [==============================] - 62s 9ms/step - loss: 1051.5784 - val_loss: 19.2871 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "6506/6506 [==============================] - 57s 9ms/step - loss: 335.9425 - val_loss: 39.1898 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "6506/6506 [==============================] - 54s 8ms/step - loss: 274.9128 - val_loss: 79.6180 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 245.8609 - val_loss: 13.9030 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 237.3326 - val_loss: 11.2595 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 217.0293 - val_loss: 193.5535 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 196.4398 - val_loss: 15.1233 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 188.1384 - val_loss: 58.3274 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 174.1415 - val_loss: 26.5835 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "6506/6506 [==============================] - 92s 14ms/step - loss: 182.0835 - val_loss: 18.4195 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "6506/6506 [==============================] - 279s 43ms/step - loss: 108.8555 - val_loss: 10.1869 - lr: 2.0000e-04\n",
      "Epoch 12/150\n",
      "6506/6506 [==============================] - 180s 28ms/step - loss: 108.9457 - val_loss: 10.0348 - lr: 2.0000e-04\n",
      "Epoch 13/150\n",
      "6506/6506 [==============================] - 1034s 159ms/step - loss: 109.5297 - val_loss: 9.9191 - lr: 2.0000e-04\n",
      "Epoch 14/150\n",
      "6506/6506 [==============================] - 3069s 472ms/step - loss: 106.3509 - val_loss: 10.9507 - lr: 2.0000e-04\n",
      "Epoch 15/150\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 105.2147 - val_loss: 9.7947 - lr: 2.0000e-04\n",
      "Epoch 16/150\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 106.0102 - val_loss: 9.6182 - lr: 2.0000e-04\n",
      "Epoch 17/150\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 105.9715 - val_loss: 9.4532 - lr: 2.0000e-04\n",
      "Epoch 18/150\n",
      "6506/6506 [==============================] - 727s 112ms/step - loss: 106.1470 - val_loss: 11.8714 - lr: 2.0000e-04\n",
      "Epoch 19/150\n",
      "6506/6506 [==============================] - 761s 117ms/step - loss: 105.2807 - val_loss: 10.0889 - lr: 2.0000e-04\n",
      "Epoch 20/150\n",
      "6506/6506 [==============================] - 309s 47ms/step - loss: 104.6010 - val_loss: 9.6858 - lr: 2.0000e-04\n",
      "Epoch 21/150\n",
      "6506/6506 [==============================] - 54s 8ms/step - loss: 102.8629 - val_loss: 10.5446 - lr: 2.0000e-04\n",
      "Epoch 22/150\n",
      "6506/6506 [==============================] - 54s 8ms/step - loss: 103.3480 - val_loss: 11.2347 - lr: 2.0000e-04\n",
      "Epoch 23/150\n",
      "6506/6506 [==============================] - 56s 9ms/step - loss: 97.5627 - val_loss: 9.6712 - lr: 1.0000e-04\n",
      "Epoch 24/150\n",
      "6506/6506 [==============================] - 54s 8ms/step - loss: 97.0774 - val_loss: 9.2993 - lr: 1.0000e-04\n",
      "Epoch 25/150\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 97.6373 - val_loss: 9.1972 - lr: 1.0000e-04\n",
      "Epoch 26/150\n",
      "6506/6506 [==============================] - 54s 8ms/step - loss: 96.9792 - val_loss: 10.0318 - lr: 1.0000e-04\n",
      "Epoch 27/150\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 97.2430 - val_loss: 9.4301 - lr: 1.0000e-04\n",
      "Epoch 28/150\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 96.3341 - val_loss: 18.0357 - lr: 1.0000e-04\n",
      "Epoch 29/150\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 96.9017 - val_loss: 9.6697 - lr: 1.0000e-04\n",
      "Epoch 30/150\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 95.9382 - val_loss: 9.2397 - lr: 1.0000e-04\n",
      "Epoch 31/150\n",
      "6506/6506 [==============================] - 58s 9ms/step - loss: 96.5934 - val_loss: 9.2866 - lr: 1.0000e-04\n",
      "Epoch 32/150\n",
      "6506/6506 [==============================] - 57s 9ms/step - loss: 97.0892 - val_loss: 9.3514 - lr: 1.0000e-04\n",
      "Epoch 33/150\n",
      "6506/6506 [==============================] - 59s 9ms/step - loss: 96.1558 - val_loss: 9.2861 - lr: 1.0000e-04\n",
      "Epoch 34/150\n",
      "6506/6506 [==============================] - 54s 8ms/step - loss: 95.6986 - val_loss: 9.3003 - lr: 1.0000e-04\n",
      "Epoch 35/150\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 95.6239 - val_loss: 9.0575 - lr: 1.0000e-04\n",
      "Epoch 36/150\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 95.8268 - val_loss: 9.0078 - lr: 1.0000e-04\n",
      "Epoch 37/150\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 95.1074 - val_loss: 9.7340 - lr: 1.0000e-04\n",
      "Epoch 38/150\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 94.5196 - val_loss: 9.5955 - lr: 1.0000e-04\n",
      "Epoch 39/150\n",
      "6506/6506 [==============================] - 58s 9ms/step - loss: 95.5265 - val_loss: 10.3393 - lr: 1.0000e-04\n",
      "Epoch 40/150\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 94.7351 - val_loss: 8.8747 - lr: 1.0000e-04\n",
      "Epoch 41/150\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 95.5668 - val_loss: 8.8268 - lr: 1.0000e-04\n",
      "Epoch 42/150\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 94.8275 - val_loss: 10.5878 - lr: 1.0000e-04\n",
      "Epoch 43/150\n",
      "6506/6506 [==============================] - 54s 8ms/step - loss: 94.5091 - val_loss: 8.9862 - lr: 1.0000e-04\n",
      "Epoch 44/150\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 94.6311 - val_loss: 9.6219 - lr: 1.0000e-04\n",
      "Epoch 45/150\n",
      "6506/6506 [==============================] - 180s 28ms/step - loss: 94.7310 - val_loss: 8.7654 - lr: 1.0000e-04\n",
      "Epoch 46/150\n",
      "6506/6506 [==============================] - 159s 25ms/step - loss: 94.3761 - val_loss: 8.8271 - lr: 1.0000e-04\n",
      "Epoch 47/150\n",
      "6506/6506 [==============================] - 575s 88ms/step - loss: 94.7021 - val_loss: 8.8820 - lr: 1.0000e-04\n",
      "Epoch 48/150\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 95.4783 - val_loss: 9.1478 - lr: 1.0000e-04\n",
      "Epoch 49/150\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 93.4613 - val_loss: 8.8614 - lr: 1.0000e-04\n",
      "Epoch 50/150\n",
      "6506/6506 [==============================] - 57s 9ms/step - loss: 94.1019 - val_loss: 8.9348 - lr: 1.0000e-04\n",
      "Epoch 51/150\n",
      "6506/6506 [==============================] - 53s 8ms/step - loss: 94.1348 - val_loss: 9.4662 - lr: 1.0000e-04\n",
      "Epoch 52/150\n",
      "6506/6506 [==============================] - 52s 8ms/step - loss: 93.4833 - val_loss: 8.7645 - lr: 1.0000e-04\n",
      "Epoch 53/150\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 94.1819 - val_loss: 8.8279 - lr: 1.0000e-04\n",
      "Epoch 54/150\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 93.8478 - val_loss: 9.6401 - lr: 1.0000e-04\n",
      "Epoch 55/150\n",
      "6506/6506 [==============================] - 50s 8ms/step - loss: 93.9868 - val_loss: 8.6915 - lr: 1.0000e-04\n",
      "Epoch 56/150\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 92.9212 - val_loss: 8.7967 - lr: 1.0000e-04\n",
      "Epoch 57/150\n",
      "6506/6506 [==============================] - 50s 8ms/step - loss: 93.8274 - val_loss: 9.8174 - lr: 1.0000e-04\n",
      "Epoch 58/150\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 93.3829 - val_loss: 8.7184 - lr: 1.0000e-04\n",
      "Epoch 59/150\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 93.7119 - val_loss: 8.7133 - lr: 1.0000e-04\n",
      "Epoch 60/150\n",
      "6506/6506 [==============================] - 50s 8ms/step - loss: 93.7593 - val_loss: 9.3009 - lr: 1.0000e-04\n",
      "Epoch 61/150\n",
      "6506/6506 [==============================] - 50s 8ms/step - loss: 93.5424 - val_loss: 9.2771 - lr: 1.0000e-04\n",
      "Epoch 62/150\n",
      "6506/6506 [==============================] - 50s 8ms/step - loss: 93.9158 - val_loss: 8.5996 - lr: 1.0000e-04\n",
      "Epoch 63/150\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 91.9936 - val_loss: 8.8936 - lr: 1.0000e-04\n",
      "Epoch 64/150\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 93.6083 - val_loss: 8.8436 - lr: 1.0000e-04\n",
      "Epoch 65/150\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 93.6105 - val_loss: 9.3585 - lr: 1.0000e-04\n",
      "Epoch 66/150\n",
      "6506/6506 [==============================] - 50s 8ms/step - loss: 93.4154 - val_loss: 8.6014 - lr: 1.0000e-04\n",
      "Epoch 67/150\n",
      "6506/6506 [==============================] - 50s 8ms/step - loss: 92.6172 - val_loss: 11.2678 - lr: 1.0000e-04\n",
      "Epoch 68/150\n",
      "6506/6506 [==============================] - 50s 8ms/step - loss: 92.4645 - val_loss: 8.7825 - lr: 1.0000e-04\n",
      "Epoch 69/150\n",
      "6506/6506 [==============================] - 50s 8ms/step - loss: 92.5821 - val_loss: 9.2571 - lr: 1.0000e-04\n",
      "Epoch 70/150\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 92.6209 - val_loss: 9.1794 - lr: 1.0000e-04\n",
      "Epoch 71/150\n",
      "6506/6506 [==============================] - 50s 8ms/step - loss: 92.7454 - val_loss: 12.1747 - lr: 1.0000e-04\n",
      "Epoch 72/150\n",
      "6506/6506 [==============================] - 50s 8ms/step - loss: 92.2309 - val_loss: 9.3862 - lr: 1.0000e-04\n",
      "Epoch 73/150\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 93.8544 - val_loss: 8.6253 - lr: 1.0000e-04\n",
      "Epoch 74/150\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 92.9346 - val_loss: 8.6971 - lr: 1.0000e-04\n",
      "Epoch 75/150\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 91.5844 - val_loss: 8.7195 - lr: 1.0000e-04\n",
      "Epoch 76/150\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 92.5757 - val_loss: 8.6217 - lr: 1.0000e-04\n",
      "Epoch 77/150\n",
      "6506/6506 [==============================] - 51s 8ms/step - loss: 92.5569 - val_loss: 8.6048 - lr: 1.0000e-04\n",
      "14013/14013 [==============================] - 16s 1ms/step\n",
      "MAE: 3.7497055825741423, RMSE: 17.728842553193832\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(16,activation='relu'),\n",
    "    Dense(8,activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "#compile model\n",
    "model.compile(optimizer=Adam(), loss='mse')\n",
    "\n",
    "#train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=150,\n",
    "                    batch_size=248,  # Start with a larger batch size\n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "#model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# evaluate on test set\n",
    "predictions = model.predict(X_test)\n",
    "predictions = predictions.squeeze()\n",
    "evaluate_model(predictions, y_test)#evaluate model accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c420ea68-7145-4ee8-980d-6d0d768649eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train/test sequences\n",
    "sequence_length = 35 # holds the length of each sequence\n",
    "X_train, y_train = create_sequences(scaled_train, train_df['target'], sequence_length)\n",
    "X_test, y_test = create_sequences(scaled_test, test_df['target'], sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d73174f-d1a8-455e-9f60-70dec0654cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16134/16134 [==============================] - 91s 6ms/step - loss: 819.2036 - val_loss: 23.8225 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "16134/16134 [==============================] - 75s 5ms/step - loss: 292.8583 - val_loss: 16.1900 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "16134/16134 [==============================] - 71s 4ms/step - loss: 274.7835 - val_loss: 14.2757 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "16134/16134 [==============================] - 71s 4ms/step - loss: 233.6570 - val_loss: 29.8696 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "16134/16134 [==============================] - 71s 4ms/step - loss: 223.4670 - val_loss: 12.9636 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "16134/16134 [==============================] - 72s 4ms/step - loss: 218.0679 - val_loss: 17.8999 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "16134/16134 [==============================] - 78s 5ms/step - loss: 188.8941 - val_loss: 24.1921 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "16134/16134 [==============================] - 72s 4ms/step - loss: 187.8905 - val_loss: 13.3859 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "16134/16134 [==============================] - 69s 4ms/step - loss: 178.5831 - val_loss: 13.8897 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "16134/16134 [==============================] - 68s 4ms/step - loss: 166.6600 - val_loss: 19.8330 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "16134/16134 [==============================] - 69s 4ms/step - loss: 158.9793 - val_loss: 14.9869 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "16134/16134 [==============================] - 70s 4ms/step - loss: 163.7711 - val_loss: 13.0590 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "16134/16134 [==============================] - 72s 4ms/step - loss: 153.0899 - val_loss: 11.1682 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "16134/16134 [==============================] - 74s 5ms/step - loss: 154.3936 - val_loss: 13.7187 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "16134/16134 [==============================] - 79s 5ms/step - loss: 151.2747 - val_loss: 11.5465 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "16134/16134 [==============================] - 75s 5ms/step - loss: 154.5992 - val_loss: 50.5826 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "16134/16134 [==============================] - 68s 4ms/step - loss: 149.2961 - val_loss: 11.0660 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "16134/16134 [==============================] - 67s 4ms/step - loss: 148.8385 - val_loss: 12.4352 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "16134/16134 [==============================] - 69s 4ms/step - loss: 140.7586 - val_loss: 9.9233 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "16134/16134 [==============================] - 70s 4ms/step - loss: 142.7537 - val_loss: 9.8637 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "16134/16134 [==============================] - 71s 4ms/step - loss: 135.6519 - val_loss: 21.8589 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "16134/16134 [==============================] - 70s 4ms/step - loss: 132.6934 - val_loss: 9.8248 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "16134/16134 [==============================] - 73s 5ms/step - loss: 137.5932 - val_loss: 16.0412 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "16134/16134 [==============================] - 68s 4ms/step - loss: 128.7560 - val_loss: 16.5691 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "16134/16134 [==============================] - 68s 4ms/step - loss: 130.1037 - val_loss: 9.7826 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "16134/16134 [==============================] - 68s 4ms/step - loss: 131.2953 - val_loss: 9.3629 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "16134/16134 [==============================] - 67s 4ms/step - loss: 128.7414 - val_loss: 31.9348 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "16134/16134 [==============================] - 69s 4ms/step - loss: 128.2074 - val_loss: 9.6654 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "16134/16134 [==============================] - 69s 4ms/step - loss: 125.8640 - val_loss: 10.5062 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "16134/16134 [==============================] - 70s 4ms/step - loss: 126.5376 - val_loss: 9.7541 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "16134/16134 [==============================] - 69s 4ms/step - loss: 127.6411 - val_loss: 12.2421 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "16134/16134 [==============================] - 77s 5ms/step - loss: 126.4914 - val_loss: 10.0275 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "16134/16134 [==============================] - 74s 5ms/step - loss: 120.5500 - val_loss: 10.4452 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "16134/16134 [==============================] - 70s 4ms/step - loss: 123.0805 - val_loss: 10.1355 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "16134/16134 [==============================] - 69s 4ms/step - loss: 121.5719 - val_loss: 12.4269 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "16134/16134 [==============================] - 71s 4ms/step - loss: 117.1588 - val_loss: 10.0752 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "16134/16134 [==============================] - 74s 5ms/step - loss: 118.6637 - val_loss: 9.3854 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "16134/16134 [==============================] - 69s 4ms/step - loss: 126.0346 - val_loss: 10.2464 - lr: 0.0010\n",
      "Epoch 39/100\n",
      "16134/16134 [==============================] - 69s 4ms/step - loss: 121.0363 - val_loss: 9.1108 - lr: 0.0010\n",
      "Epoch 40/100\n",
      "16134/16134 [==============================] - 70s 4ms/step - loss: 123.3396 - val_loss: 9.0869 - lr: 0.0010\n",
      "Epoch 41/100\n",
      "16134/16134 [==============================] - 69s 4ms/step - loss: 118.2711 - val_loss: 10.5741 - lr: 0.0010\n",
      "Epoch 42/100\n",
      "16134/16134 [==============================] - 71s 4ms/step - loss: 121.3776 - val_loss: 20.6328 - lr: 0.0010\n",
      "Epoch 43/100\n",
      "16134/16134 [==============================] - 77s 5ms/step - loss: 122.8660 - val_loss: 9.1325 - lr: 0.0010\n",
      "Epoch 44/100\n",
      "16134/16134 [==============================] - 73s 5ms/step - loss: 117.3969 - val_loss: 9.0468 - lr: 0.0010\n",
      "Epoch 45/100\n",
      "16134/16134 [==============================] - 68s 4ms/step - loss: 115.8480 - val_loss: 9.7995 - lr: 0.0010\n",
      "Epoch 46/100\n",
      "16134/16134 [==============================] - 68s 4ms/step - loss: 122.9181 - val_loss: 9.1295 - lr: 0.0010\n",
      "Epoch 47/100\n",
      "16134/16134 [==============================] - 69s 4ms/step - loss: 117.5621 - val_loss: 9.2095 - lr: 0.0010\n",
      "Epoch 48/100\n",
      "16134/16134 [==============================] - 71s 4ms/step - loss: 117.8653 - val_loss: 10.2388 - lr: 0.0010\n",
      "Epoch 49/100\n",
      "16134/16134 [==============================] - 70s 4ms/step - loss: 117.9400 - val_loss: 9.2930 - lr: 0.0010\n",
      "Epoch 50/100\n",
      "16134/16134 [==============================] - 73s 5ms/step - loss: 116.3977 - val_loss: 8.9734 - lr: 0.0010\n",
      "Epoch 51/100\n",
      "16134/16134 [==============================] - 69s 4ms/step - loss: 112.6008 - val_loss: 9.7136 - lr: 0.0010\n",
      "Epoch 52/100\n",
      "16134/16134 [==============================] - 73s 5ms/step - loss: 119.9903 - val_loss: 9.8956 - lr: 0.0010\n",
      "Epoch 53/100\n",
      "16134/16134 [==============================] - 77s 5ms/step - loss: 115.3495 - val_loss: 9.6195 - lr: 0.0010\n",
      "Epoch 54/100\n",
      "16134/16134 [==============================] - 72s 4ms/step - loss: 117.4795 - val_loss: 9.5209 - lr: 0.0010\n",
      "Epoch 55/100\n",
      "16134/16134 [==============================] - 68s 4ms/step - loss: 117.0682 - val_loss: 8.6634 - lr: 0.0010\n",
      "Epoch 56/100\n",
      "16134/16134 [==============================] - 69s 4ms/step - loss: 113.4359 - val_loss: 18.7248 - lr: 0.0010\n",
      "Epoch 57/100\n",
      "16134/16134 [==============================] - 69s 4ms/step - loss: 112.9798 - val_loss: 9.0103 - lr: 0.0010\n",
      "Epoch 58/100\n",
      "16134/16134 [==============================] - 68s 4ms/step - loss: 118.3108 - val_loss: 10.2746 - lr: 0.0010\n",
      "Epoch 59/100\n",
      "16134/16134 [==============================] - 68s 4ms/step - loss: 123.6800 - val_loss: 9.8212 - lr: 0.0010\n",
      "Epoch 60/100\n",
      "16134/16134 [==============================] - 68s 4ms/step - loss: 113.4236 - val_loss: 10.4814 - lr: 0.0010\n",
      "Epoch 61/100\n",
      "16134/16134 [==============================] - 68s 4ms/step - loss: 116.9380 - val_loss: 8.8532 - lr: 0.0010\n",
      "Epoch 62/100\n",
      "16134/16134 [==============================] - 73s 5ms/step - loss: 113.6734 - val_loss: 9.2934 - lr: 0.0010\n",
      "Epoch 63/100\n",
      "16134/16134 [==============================] - 68s 4ms/step - loss: 114.3454 - val_loss: 9.1796 - lr: 0.0010\n",
      "Epoch 64/100\n",
      "16134/16134 [==============================] - 68s 4ms/step - loss: 110.4283 - val_loss: 8.8771 - lr: 0.0010\n",
      "Epoch 65/100\n",
      "16134/16134 [==============================] - 67s 4ms/step - loss: 113.1909 - val_loss: 12.2788 - lr: 0.0010\n",
      "Epoch 66/100\n",
      "16134/16134 [==============================] - 71s 4ms/step - loss: 112.7402 - val_loss: 9.4299 - lr: 0.0010\n",
      "Epoch 67/100\n",
      "16134/16134 [==============================] - 68s 4ms/step - loss: 109.5094 - val_loss: 16.1696 - lr: 0.0010\n",
      "Epoch 68/100\n",
      "16134/16134 [==============================] - 67s 4ms/step - loss: 112.6118 - val_loss: 9.1608 - lr: 0.0010\n",
      "Epoch 69/100\n",
      "16134/16134 [==============================] - 71s 4ms/step - loss: 110.8377 - val_loss: 9.0588 - lr: 0.0010\n",
      "Epoch 70/100\n",
      "16134/16134 [==============================] - 69s 4ms/step - loss: 109.9801 - val_loss: 9.0272 - lr: 0.0010\n",
      "14013/14013 [==============================] - 15s 1ms/step\n",
      "MAE: 3.7941385729475194, RMSE: 18.050898459372252\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    #Dense(128, activation='sigmoid'),\n",
    "    Dense(64, activation='relu'),\n",
    "    #Dense(64, activation='sigmoid'),\n",
    "    Dense(32,activation='relu'),\n",
    "    #Dense(32,activation='sigmoid'),\n",
    "    Dense(16,activation='relu'),\n",
    "    #Dense(8,activation='sigmoid'),\n",
    "    Flatten(),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "#compile model\n",
    "model.compile(optimizer=Adam(), loss='mse')\n",
    "\n",
    "#train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,  # Start with a larger batch size\n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "#model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# evaluate on test set\n",
    "predictions = model.predict(X_test)\n",
    "predictions = predictions.squeeze()\n",
    "evaluate_model(predictions, y_test)#evaluate model accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eeb02c-6f55-4a47-952e-d5faacbb9d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
